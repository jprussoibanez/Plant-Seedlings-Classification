{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/jprussoibanez/plant-seedlings-classification/blob/master/Plant%20Seedlings%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bFaXLFpsVqGG"},"source":["### IMPORTANT: Change BASE_DATASET_FOLDER with the root dataset folder with the images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","# Chack environment: google colab or kaggle/local\n","IN_GOOGLE_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if IN_GOOGLE_COLAB:    \n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Change BASE_DATASET_FOLDER according to folder with dataset\n","    BASE_DATASET_FOLDER = \"/content/drive/My Drive/Colab Notebooks/Tryolabs-CV-Interview/data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"colab_type":"code","id":"_2UIXqgKVgH3","outputId":"f837bf3f-e358-40c3-ad5e-28a67461084b","trusted":true},"outputs":[],"source":["if not IN_GOOGLE_COLAB:\n","    # Kaggle or local environment\n","    # Change BASE_DATASET_FOLDER according to folder with dataset\n","    BASE_DATASET_FOLDER = \"../input/plant-seedlings-classification\""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KvS6f_7BQsZq"},"source":["# The project\n","The problem to analyze is the [Kaggle plant seedlings classification](https://www.kaggle.com/c/plant-seedlings-classification).\n","\n","##  Competition goal\n","The goal is to differentiate a weed from a crop seedling in order to perform site-specific weed control.\n","\n","## Dataset\n","The [database of images](https://arxiv.org/abs/1711.05458) has approximately 960 unique plants belonging to 12 species at several growth stages. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\n","\n","## Kernel structure\n","The following is a summary of the kernel main structure.\n","\n","### 1. Kagglers challenges and discussions\n","- Review other kagglers kernels to better understand the competition challenges.\n","- Review the competition discussion forum for interesting conversation threads.\n","- Generate insights from the information gathering.\n","\n","For further information please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/kagglers_discussions.md).\n","\n","### 2. Libraries and settings\n","- This section has available settings to configure the model and its training parameters.\n","\n","For further information please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/settings.md).\n","\n","### 3. Data analysis\n","- Data exploration and descriptive analysis to determine dataset shape and distribution.\n","- Use of t-SNE to reduce dimensionality for data visualization.\n","\n","### 4. Pre-processing\n","- Class weights calculation to balance the dataset distribution.\n","- Image segmentation to mask image background.\n","- Data augmentation to increase the images dataset.\n","\n","### 5. Processing\n","- Use of transfer learning with different pre-trained models like Resnet50 and InceptionV3. Other pre-trained models can be easily added.\n","- Use of custom CNN with multiple layers.\n","- FNN as the last layer classifier.\n","\n","### 6. Generate prediction file\n","- Generate prediction file with Kaggle competition format."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0SIwnKrsQsZt"},"source":["# 1. Kagglers challenges and discussions\n","The first step is to review the discussion forum for this competition to better discover the challenges and approaches in solving the problem.\n","\n","For a detailed discussion please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/kagglers_discussions.md).\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"evTpu-B-QsZu"},"source":["# 2. Libraries and settings\n","\n","## 2.1 Libraries\n","\n","This are the main libraries for coding the kernel:\n","1. [tensorflow](https://www.tensorflow.org/) with [keras](https://keras.io/) for managing the deep learning models. Using latest tensorflow 2.0 and tensorflow keras on google colab.\n","2. [sckilit-learn](https://scikit-learn.org/), [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/) with [seaborn](https://seaborn.pydata.org/) for data manipulation, analysis and visualization.\n","3. [TQDM](https://github.com/tqdm/tqdm) for progress bar visualization on processing.\n","4. [opencv](https://pypi.org/project/opencv-python/) for image processing."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"XJdpDXZYQsZv","outputId":"18dd7850-1732-4c28-9ae7-e26debae11da","trusted":true},"outputs":[],"source":["if IN_GOOGLE_COLAB:\n","  !pip install scikit-optimize\n","  %tensorflow_version 2.x\n","\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","# Keras\n","from tensorflow.keras.applications import vgg16, vgg19, inception_v3, resnet_v2, xception, resnet, inception_resnet_v2, nasnet, mobilenet_v2\n","from tensorflow.keras.optimizers import SGD, Adam, Adadelta\n","from tensorflow.keras.regularizers import l1, l2, l1_l2\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, GlobalAveragePooling2D, Input, Activation, BatchNormalization, GlobalMaxPooling2D, Lambda\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n","from tensorflow.keras.models import load_model\n","\n","import os\n","\n","# Helpers\n","from tqdm import tqdm\n","from enum import Enum\n","\n","# Image processing\n","import cv2\n","\n","# Image plotting\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","%matplotlib inline\n","\n","# Sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import StratifiedKFold\n","from skopt import BayesSearchCV\n","from skopt.space import Real, Categorical, Integer\n","\n","import xgboost as xgb\n","from sklearn.svm import SVC\n","import lightgbm as lgb"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r8LcDxHkQsZz"},"source":["## 2.2 Global variables\n","\n","This are the global variables use to setup the configuration for the models."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConvolutionalModel(Enum):\n","    CUSTOM_CNN = 1    \n","    RESNET_V2 = 2     \n","    INCEPTION_V3 = 3  \n","    XCEPTION = 4      \n","    INCEPTION_RESNET_V2 =  5\n","    VGG16 = 6\n","    VGG19 = 7\n","    MOBILE_NET_V2 = 8\n","    NASNET_MOBILE = 9\n","    LOAD_MODEL = 10\n","\n","class ClassifierModel(Enum):\n","    FCN = 1    \n","    XGBoost = 2     \n","    SVC = 3\n","    LIGHT_GBM = 4\n","    BEST_MODEL_SVC = 5\n","    BEST_MODEL_XGBoost = 6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"qs5mCfiQQsZz","trusted":true},"outputs":[],"source":["# Global settings\n","TRAIN_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"train\")\n","TEST_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"test\")\n","IMAGE_WIDTH = 224\n","IMAGE_HEIGHT = 224\n","IMAGE_CHANNELS = 3\n","\n","TSNE_VISUALIZATION = True\n","\n","#Name to load convolutional model. This models are available on a Kaggle's dataset.\n","#CONVOLUTIONAL_MODEL_WEIGHTS_PATH = '../input/plantseedlingsclassificationmodel/weights_vgg19.h5'\n","#CONVOLUTIONAL_MODEL_WEIGHTS_PATH = '../input/plantseedlingsclassificationmodel/weights_xception.h5'\n","CONVOLUTIONAL_MODEL_WEIGHTS_PATH = '../input/plantseedlingsclassificationmodel/weights_resnet_v2.h5'\n","LOAD_MODEL_PREPROCESS_FUNCTION = resnet_v2.preprocess_input\n","\n","# Data augmentation settings\n","rotation_range = 180      \n","zoom_range = 0.5      \n","width_shift_range = 0.5  \n","height_shift_range = 0.5 \n","horizontal_flip = True   \n","vertical_flip = True\n","\n","# Training settings\n","batch_size = 32\n","epochs = 500\n","patience = 10\n","\n","# Choose convolutional base and classifier model to train\n","CONVOLUTIONAL_MODEL = ConvolutionalModel.VGG19\n","CLASSIFIER_MODEL = ClassifierModel.FCN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VUpWTJIOQsZ3"},"source":["# 3. Data analysis\n","\n","The first and the most important task in solving a problem with Machine Learning is to analyze the dataset before proceeding with any algorithms. This is important in order to understand the complexity of the dataset which will eventually help in designing the algorithm."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ky0fNzmqQsZ3"},"source":["## 3.1 Load data and images\n","\n","The first step is to load the images and data from the datasets through panda and dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"IYJgzEtUQsZ4","trusted":true},"outputs":[],"source":["def get_species_groups():\n","    \"\"\" \n","    Get the species group from the folders' name on the dataset. \n","  \n","    @Returns: \n","        array: Array of species groups to train.\n","    \"\"\"\n","    return [\n","      'Black-grass',\n","      'Charlock',\n","      'Cleavers',\n","      'Common Chickweed',\n","      'Common wheat',\n","      'Fat Hen',\n","      'Loose Silky-bent',\n","      'Maize',\n","      'Scentless Mayweed',\n","      'Shepherds Purse',\n","      'Small-flowered Cranesbill',\n","      'Sugar beet'\n","    ]\n","\n","def read_train_data(species_groups):\n","    \"\"\" \n","    Read the train species data from the datasets folder. \n","  \n","    @Parameters:\n","        species_groups (array): Array of species groups to train.\n","  \n","    @Returns: \n","        dataframe: Dataframe with the filepath, filename and species group category for each species image.\n","    \"\"\"\n","    train = []\n","    for _, species in tqdm(enumerate(species_groups), total=len(species_groups)):\n","        for file in os.listdir(os.path.join(TRAIN_DATASET_FOLDER, species)):\n","            train.append([f'{TRAIN_DATASET_FOLDER}/{species}/{file}', file, species])\n","    \n","    train_df = pd.DataFrame(train, columns=['filepath', 'file', 'species'])\n","    \n","    return train_df\n","\n","def read_image(image_file_path, image_width, image_height):\n","    \"\"\" \n","    Read and transform image according to widht, height and channels.\n","  \n","    @Parameters:\n","        image_file_path(string): Image file path to read.\n","        image_width (int): Image width to resize.\n","        image_height (int): Image height to resize.\n","  \n","    @Returns: \n","        array: Array of loaded images resized to image_width and image_height.\n","    \"\"\"\n","    image = cv2.imread(image_file_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_width, image_height))\n","    \n","    return image\n","\n","def load_images(filepaths, image_width, image_height, preprocess_function = lambda x: x):\n","    images =[]\n","    for filepath in tqdm(filepaths, total=len(filepaths)):\n","      images.append(preprocess_function(read_image(filepath, image_width, image_height)))\n","\n","    return images\n","    \n","def load_species_images(species_data, image_width, image_height, preprocess_function = lambda x: x):\n","    \"\"\" \n","    Load images to input features on neural network model and do one hot encode for labels.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        image_width (int): Image width to resize.\n","        image_height (int): Image height to resize.\n","       \n","    @Returns: \n","        species_X (array): Array of images on the data.\n","        species_Y (array): One hot encode for labels.\n","    \"\"\" \n","    species_X = load_images(species_data['filepath'],image_width, image_height, preprocess_function)\n","    species_Y = pd.get_dummies(species_data['species'], drop_first = False)\n","    \n","    return np.stack(species_X), species_Y\n","\n","def show_grid_multiple_sample(species_data, species_groups):\n","    \"\"\" \n","    Plot sample images from each species group.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        species_groups (array): Array with species group.\n","       \n","    @Returns: None\n","    Plots grid with number_of_samples images with image_size for each species_group\n","    \"\"\"\n","    number_of_samples = 8\n","    image_size = 100\n","    \n","    number_of_species = len(species_groups) \n","    fig, axes = plt.subplots(nrows = number_of_species, ncols = number_of_samples + 1, figsize = (20, 28), gridspec_kw = {'wspace': 0.05, 'hspace': 0})\n"," \n","    for species_id, species in enumerate(species_groups):\n","        samples_species_filepath = species_data[species_data['species'] == species]['filepath'].sample(number_of_samples)\n","        ax = axes[species_id, 0]\n","        ax.axis('off')\n","        ax.text(0.5, 0.5, species, horizontalalignment = 'center', verticalalignment = 'center', fontsize = 10, transform = ax.transAxes)\n","        for sample_id, sample_species_filepath in enumerate(samples_species_filepath):\n","            image = read_image(sample_species_filepath, image_size, image_size)\n","            ax = axes[species_id, sample_id + 1]\n","            ax.axis('off')\n","            ax.imshow(image)\n","            \n","    plt.show()\n","\n","def show_grid_one_sample(species_data, species_groups):\n","    \"\"\" \n","    Plot one sample image from each species group.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        species_groups (array): Array with species group.\n","       \n","    @Returns: None\n","    Plots one sample image with image_size for each species_group\n","    \"\"\"\n","    image_size = 100\n","    number_of_species = len(species_groups)\n","    \n","    fig, axes = plt.subplots(nrows = 2, ncols = int(number_of_species / 2), figsize = (20, 8), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n","    axes = axes.flatten()\n","    \n","    for group_id, (group_name, group) in enumerate(species_data.groupby('species')):\n","        image = read_image(group.sample(1).iloc[0]['filepath'], image_size, image_size)\n","        axes[group_id].axis('off')\n","        axes[group_id].set_title(group_name)\n","        axes[group_id].imshow(image)\n","            \n","    plt.show()\n","    \n","def show_species_distribution(species_data):\n","    \"\"\" \n","    Plot species distribution.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","       \n","    @Returns: None\n","    Plots species distribution on an histogram and pie chart.\n","    \"\"\"\n","    species_groups = species_data['species'].unique()\n","    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n","    colors_palette = sns.husl_palette(len(species_groups), h = 0.01, l = 0.6, s = .2)\n","    explode_chart = np.full(len(species_groups), 0.1)\n","    \n","    species_data.groupby('species').size().plot.bar(\n","        subplots = True, ax = axes[0], stacked = True, title = 'Species distribution', label = \"\", color = colors_palette\n","    )\n","     \n","    species_data['species'].value_counts().plot.pie(\n","        subplots = True, ax = axes[1], autopct = '%.2f', title = 'Species distribution', label = \"\", colors = colors_palette, explode = explode_chart\n","    )\n","    \n","    plt.show()\n","    \n","class TSNE_dimension(Enum):\n","    TWO = 1\n","    THREE = 2\n","    \n","def apply_PCA(images, n_components = 100):\n","    pca = PCA(n_components = n_components)\n","    pca_result = pca.fit_transform(images)\n","    \n","    return pca_result\n","\n","def apply_TSNE(images, n_components = 2, perplexity = 100):\n","    tsne = TSNE(n_components = n_components, perplexity = perplexity)\n","    tsne_result = tsne.fit_transform(images)\n","    \n","    return tsne_result\n","\n","def plot_TSNE_with_PCA(images, image_labels, dimension = TSNE_dimension.TWO):\n","    pca_results = apply_PCA(images)\n","    n_components = (2 if dimension == TSNE_dimension.TWO else 3)\n","    tsne_result = apply_TSNE(pca_results, n_components)\n","    plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO)\n","\n","def plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO):\n","    df = pd.DataFrame(tsne_result).assign(label = image_labels)\n","    plt.figure(figsize=(10,10))\n","    if dimension == TSNE_dimension.TWO:\n","        sns.scatterplot(x = 0, y = 1, hue = 'label', data = df);\n","    else:\n","        fig = px.scatter_3d(df, x = 0, y = 1, z = 2, color = 'label')\n","        fig.show();"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dzC2PaiDQsZ8"},"source":["### 3.1.1 Load data\n","\n","Load data from the datasets image folders."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"D47VH-A2QsZ9","outputId":"85940d1f-4680-4959-8ee6-0fadbe148545","trusted":true},"outputs":[],"source":["species_groups = get_species_groups()\n","print(f\"This are the groups to be loaded and trained: {species_groups}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"colab_type":"code","id":"_7bv4AqEQsZ_","outputId":"ed6d6de7-dfd5-45b7-eedb-2c406c086178","trusted":true},"outputs":[],"source":["all_train_df = read_train_data(species_groups)\n","all_train_df.sample(4)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9aHsC_LjQsaC"},"source":["### 3.1.2 Descriptive analysis\n","\n","The descriptive analysis has the following steps:\n","1. Visualize sample images by species group.\n","2. Analyze species group distribution.\n","3. Apply dimensionality reduction through PCA & t-SNE."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2gKtKCyzQsaC"},"source":["#### 3.1.2.1 Sample images by species group\n","\n","The sample visualization shows the difficulty to distinguish between different weeds species groups even for a human eye."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"4EawzugWQsaD","outputId":"9ca068ac-3d11-4d51-a47a-df0669808b2d","trusted":true},"outputs":[],"source":["show_grid_multiple_sample(all_train_df, species_groups)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"colab_type":"code","id":"fNL43B3fQsaG","outputId":"f8f78c55-fc66-478b-d744-5b77df424ed1","trusted":true},"outputs":[],"source":["show_grid_one_sample(all_train_df, species_groups)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yvb4iB1IQsaJ"},"source":["#### 3.1.2.2 Species group distribution\n","\n","The distribution visualization demonstrates an imbalance in the dataset weed types or groups."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"colab_type":"code","id":"JQXjO1-AQsaJ","outputId":"fae3cb92-57ec-4064-caac-0ee532fd8135","trusted":true},"outputs":[],"source":["show_species_distribution(all_train_df)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RPYPuI69QsaM"},"source":["#### 3.1.2.3 PCA & TSNE\n","\n","The PCA and t-SNE can be applied for dimensionality reduction to images on 2D and 3D. There is no clear clustering or similarity on the lower dimensions (at least before applying the pre-processing segmentation process).\n","\n","NOTE: The number of sample images are reduced to 500 images to avoid long loading times."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"pGj3RJ6pQsaN","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    sample_df = all_train_df.sample(500)\n","    sample_images, _ = load_species_images(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"YDmQIaUFQsaP","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    flatten_images = [image.flatten() for image in sample_images]\n","    pca_results = apply_PCA(flatten_images)\n","    tsne_results = apply_TSNE(pca_results)\n","    plot_TSNE_results(tsne_results, sample_df['species'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ElWm2h5UQsaR","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    tsne_results = apply_TSNE(pca_results, n_components = 3)\n","    plot_TSNE_results(tsne_results, sample_df['species'].values, TSNE_dimension.THREE)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tz9wRNIzQsaU"},"source":["## 3.2 Conclusions\n","\n","From the descriptive analysis some of the conclusions are:\n","- There is an imbalance on the dataset which should be handled to avoid training biases.\n","- There are not much images for training deep learning models so we should use data augmentation to increase the dataset.\n","- All weed images have backgrounds that we should be removed in order to improve weed recognition.\n","- There is no clear clustering on lower dimensions before pre-processing."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8LtwOJYGQsaW"},"source":["# 4. Pre-processing\n","\n","This are the principal concerns that should be addressed on the pre-processing from the data analysis:\n","\n","1. Balance the dataset to avoid biases on imbalance weed types.\n","2. Use image segmentation to remove background as to focus on weed recognition.\n","3. Apply data augmentation to increase the data available for training."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kmaO3tQ_QsaW"},"source":["## 4.1 Balance dataset\n","\n","The class weights computation allows to handle unbalance data distribution by weighting the class importance on training.\n","\n","Other ways of balancing the dataset are oversampling and undersampling with methods like SMOTE and ADASYN."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Hy_0XXHbQsaY","trusted":true},"outputs":[],"source":["def compute_class_weights(data_groups) :\n","    return dict(enumerate(compute_class_weight(\"balanced\", np.unique(data_groups), data_groups)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"SUYTyRajQsaa","outputId":"bcdac770-b6a3-4daf-bd44-d6354f56fb89","trusted":true},"outputs":[],"source":["class_weights = compute_class_weights(all_train_df['species'])\n","print(f\"This are the class weights that will be use for training to balance the dataset: {class_weights}\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8t72_VIpQsac"},"source":["## 4.2 Segmentation\n","The segmentation process masks the main object of analysis by removing all unnecessary background.\n","\n","In this section the segmentation operation is applied to a single image with the following steps:\n","\n","1. Mask creation \n","  1. Change the color space from RGB to HSV. This is an easier space to define color ranges.\n","  2. Define the color range for green (common weed color).\n","  3. Define the most common shape for weeds. For this case close ellipses.\n","  4. Apply the morphology transformation to identify the weeds within the color range and shape. \n","2. Apply mask to remove background\n","\n","Further information can be found on openCV documentation on [colorspaces](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html) and [Morphological Transformations](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ntMbGjcNQsad","trusted":true},"outputs":[],"source":["def create_mask_for_plant(image):\n","    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    sensitivity = 35\n","    lower_hsv = np.array([60 - sensitivity, 100, 50])\n","    upper_hsv = np.array([60 + sensitivity, 255, 255])\n","\n","    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n","    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","    \n","    return mask\n","\n","def segment_plant(image):\n","    mask = create_mask_for_plant(image)\n","    output = cv2.bitwise_and(image, image, mask = mask)\n","    return output\n","\n","def sharpen_image(image):\n","    segmented = segment_plant(image)\n","    image_blurred = cv2.GaussianBlur(segmented, (0, 0), 3)\n","    image_sharp = cv2.addWeighted(segmented, 1.5, image_blurred, -0.5, 0)\n","    return image_sharp\n","\n","def create_image_tranformation(image):\n","    masked_image = create_mask_for_plant(image)\n","    segmented_image = segment_plant(image)\n","    \n","    return masked_image, segmented_image\n","\n","def load_species_images_with_segmentation(species_data, image_width, image_height):\n","    return load_species_images(species_data, image_width, image_height, segment_plant)\n","\n","def plot_image_transformation(original_image, masked_image, segmented_image):\n","    fig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(10, 10))\n","    axs[0].imshow(original_image)\n","    axs[0].set_title('Original image')\n","    axs[0].axis('off')\n","    axs[1].imshow(masked_image)\n","    axs[1].axis('off')\n","    axs[1].set_title('Masked image')\n","    axs[2].imshow(segmented_image)\n","    axs[2].set_title('Segmented image')\n","    axs[2].axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"colab_type":"code","id":"rk_SuutAQsaf","outputId":"2cac5069-0a3a-427a-9684-5f4931054752","trusted":true},"outputs":[],"source":["sample_image, _ = load_species_images(all_train_df.sample(1), IMAGE_WIDTH, IMAGE_HEIGHT)\n","masked_image, segmented_image = create_image_tranformation(sample_image[0])\n","plot_image_transformation(sample_image[0], masked_image, segmented_image)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D2iokxpcQsah"},"source":["Applying t-SNE again on the same sample images but now with segmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"kKHRFaFeQsai","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    sample_images, _ = load_species_images_with_segmentation(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)\n","    flatten_images = [image.flatten() for image in sample_images]\n","    plot_TSNE_with_PCA(flatten_images, sample_df['species'].values, TSNE_dimension.TWO)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gJOIOEYOQsal"},"source":["## 4.2 Data augmentation\n","\n","For the data augmentation we can use keras preprocessing library [ImageDataGenerator](https://keras.io/preprocessing/image/).\n","\n","The class can generate batches of tensor image data with real-time data augmentation while training the model. This are the transformations apply to augment the data:\n","- Random rotations within a range define by rotation_range.\n","- Random zooms within a range define by zoom_range.\n","- Random width shifts define by width_shift_range.\n","- Random heigh shifts define by height_shift_range.\n","- Random horizontal flips.\n","- Random vertical flips.\n","\n","NOTE: All this variables can be configured at the beginning of the notebook.\n","\n","This section will show some random transformations on a number of sample images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"bvZFauvHQsam","trusted":true},"outputs":[],"source":["def create_image_data_generator(\n","    rotation_range = 0,  \n","    zoom_range = 0,\n","    width_shift_range = 0, \n","    height_shift_range = 0, \n","    horizontal_flip = False,\n","    vertical_flip = False,\n","    rescale = None,\n","    preprocessing_function = None\n","):\n","    return ImageDataGenerator(\n","        featurewise_center = False,\n","        samplewise_center = False,\n","        featurewise_std_normalization = False,\n","        samplewise_std_normalization = False,\n","        zca_whitening = False,\n","        rotation_range = rotation_range,\n","        zoom_range = zoom_range,\n","        width_shift_range = width_shift_range,\n","        height_shift_range = height_shift_range,\n","        horizontal_flip = horizontal_flip,\n","        vertical_flip = vertical_flip,\n","        rescale = rescale,\n","        preprocessing_function = preprocessing_function\n","    )\n","    \n","def show_augmentation_row(original_image, images_iterator, number_of_augmentations, axes):   \n","    axes[0].axis('off')\n","    axes[0].set_title('Original image')\n","    axes[0].imshow(original_image)\n","    \n","    for image_number in range(number_of_augmentations):\n","        image = images_iterator.next()\n","        axes[image_number + 1].axis('off')\n","        axes[image_number + 1].set_title('Augmented image')\n","        axes[image_number + 1].imshow(image[0].astype(np.uint8))\n","\n","def show_grid_with_augmented_images(number_of_samples, number_of_augmentations, image_data_generator):   \n","    fig, axes = plt.subplots(nrows = number_of_samples, ncols = number_of_augmentations + 1, figsize = (30, 4 * number_of_samples), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n","    fig.suptitle('Sample augmented images', fontsize=16)\n","    \n","    sample_images, _ = load_species_images_with_segmentation(all_train_df.sample(number_of_samples), IMAGE_WIDTH, IMAGE_HEIGHT)\n","    for sample_id, sample_image in enumerate(sample_images):\n","        images_iterator = image_data_generator.flow(np.expand_dims(sample_image, axis=0), None, 1)\n","        show_augmentation_row(sample_image, images_iterator, number_of_augmentations, axes[sample_id])\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Qj2wffkLQsao","trusted":true},"outputs":[],"source":["image_data_generator = create_image_data_generator(\n","    rotation_range = 250,    \n","    zoom_range = 0.2,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    horizontal_flip = True,\n","    vertical_flip = True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"9Erg4XixQsaq","outputId":"9df5f9cb-3000-4584-8041-1e160199c740","trusted":true},"outputs":[],"source":["show_grid_with_augmented_images(number_of_samples = 5, number_of_augmentations = 7, image_data_generator = image_data_generator)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m0irIOlPQsas"},"source":["# 5. Processing\n","\n","This section describes the model architecture and the training process. This are the following architectures use for training:\n","\n","Convolutional model to extract features from images:\n","- **Pre-trained models**: This models are use for transfer learning to reuse their weights and model architecture. \n","- **Custom CNN**: This is a multi-layer custom CNN using batch normalization and regularization.\n","- **Load model**: A model can be loaded that was saved before with [HDF5 format](http://www.h5py.org/)\n","\n","Classifier model to classify high level features:\n","- Fully connected network: Fully connected network with dense layers, normalization, regularization and the use of softmax for classification. \n","- [XGBoost](https://xgboost.readthedocs.io/en/latest/): XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n","- [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): C-Support Vector Classification from the SVM family.\n","- [LightGBM](https://lightgbm.readthedocs.io/en/latest/): LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n","\n","For the pre-trained will use transfer learning and fine tuning as explain on [tensorflow documentaion](https://www.tensorflow.org/tutorials/images/transfer_learning).\n","\n","- **Feature Extraction**: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.You do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.\n","\n","- **Fine-Tuning**: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task.\n","\n","Also different metrics are use to validate the model like accuracy, recall, f1-score and the confusion matrix.\n","\n","The training set is divided in training (80%), validation (10%) and test sets (10%).\n","\n","The CONVOLUTIONAL_MODEL parameter (at the beginning of the notebook) can be used to select the model to train:\n","\n","- CUSTOM_CNN: Custom multi-layer CNN.\n","- RESNET_V2: Pre-trained model [Resnet_v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2).\n","- INCEPTION_V3: Pre-trained model [InceptionV3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3).\n","- XCEPTION: Pre-trained model [Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception)\n","- INCEPTION_RESNET_V2: Pre-trained model [InceptionResNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionResNetV2)\n","- VGG16: Pre-trained model [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16)\n","- VGG19: Pre-trained model [VGG19](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG19)\n","- MOBILE_NET_V2: Pre-trained model [MobileNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2)\n","- NASNET_MOBILE: Pre-trained model [NASNetMobile](https://www.tensorflow.org/api_docs/python/tf/keras/applications/NASNetMobile)\n","\n","Models can be setup with the following parameters.\n","- **preprocessing_function**: This are important for the pre-trained models which usually have a preprocess_input function. For more information please refer to the [keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications)\n","- **rescale**: Usually a typical rescaling is from [0 - 255] to [0 - 1] which works best on model training.\n","- **base_model**: Function returning the convolutional network to extract features.\n","- **classifier**: Function returning the classifier to classify features.\n","- **image_transformation**: Function to preprocess images like segmentation.\n","- **layers_to_fine_tune**: Number of layers to fine tune after training FNN classifier.\n","\n","NOTE: It should be easy to add additional pre-trained models if desired.\n","\n","Some additional improvements are:\n","\n","- ~~Use of [process_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/preprocess_input) to normalize the input with the pre-trained model data if used.~~\n","- ~~Fine tune pre-trained models by unfreezing and training the last layers on the CNN.~~\n","- ~~Use more classifiers like XGBoost, SVM, LightGBM, Random Forest, etc. instead of just the FNN.~~\n","- ~~Use hyperparameters optimization using some library like BayesSearchCV, GridSearchCV or RandomizedSearchCV to optimize model parameters.~~\n","- ~~Use cross-validation to better evaluate the estimator performance.~~\n","- ~~Stacking convolutional model with different classifiers.~~\n","- ~~Use hyperparameters optimization using some library like BayesSearchCV, GridSearchCV or RandomizedSearchCV to optimize model parameters.~~\n","- Use hyperparameters optimization for CNN and FCN like hyperopt.\n","- Ensemble models to improve performance by combining different models.\n","- Visualize [CNN filters](https://www.tensorflow.org/tutorials/generative/deepdream)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"lc4DJJRRQsat","trusted":true},"outputs":[],"source":["number_output_classes = len(get_species_groups())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"2T7CjMIDQsav","trusted":true},"outputs":[],"source":["class ConvolutionalTrainingModel:\n","    accuracy_metric = {\n","        'training': {\n","            'metric': 'accuracy',\n","            'label': 'Accuracy'\n","        },\n","        'validation': {\n","            'metric': 'val_accuracy',\n","            'label': 'Validation accuracy'\n","        }\n","    }\n","    loss_metric = {\n","        'training': {\n","            'metric': 'loss',\n","            'label': 'Loss'\n","        },\n","        'validation': {\n","            'metric': 'val_loss',\n","            'label': 'Validation loss'\n","        }\n","    }\n","    \n","    def __init__(\n","        self, \n","        image_data_generator_for_training, \n","        image_data_generator_for_validation, \n","        image_data_generator_for_prediction,\n","        base_model,\n","        classifier_model\n","    ):\n","        self.image_data_generator_for_training = image_data_generator_for_training\n","        self.image_data_generator_for_validation = image_data_generator_for_validation\n","        self.image_data_generator_for_prediction = image_data_generator_for_prediction\n","        self.optimizer = Adam(lr = 1e-3)\n","        self.fine_tune_optimizer = Adam(lr = 1e-5)\n","        self.loss = 'categorical_crossentropy'\n","        self.metrics = [ConvolutionalTrainingModel.accuracy_metric['training']['metric']]\n","        self.training_results = None\n","        self.species_names = get_species_groups()\n","        self.base_model = base_model\n","        self.extracting_features_model = None\n","        \n","        if base_model != None and classifier_model != None:\n","            self.training_model = self.create_model(base_model, classifier_model)\n","        else:\n","            self.training_model = None\n","    \n","    def create_model(self, base_model, classifier_model):\n","        model = Sequential(name = \"Species-Prediction\")\n","        model.add(base_model)\n","        model.add(classifier_model)\n","\n","        model.compile(optimizer = self.optimizer, loss = self.loss, metrics = self.metrics)  \n","        model.summary()\n","\n","        return model\n","\n","    def train_model(\n","        self,\n","        train_X,\n","        train_y,\n","        validation_data_X,\n","        validation_data_y,\n","        batch_size,\n","        epochs,\n","        patience,\n","        class_weights\n","    ):\n","        print(f\"Modeling training with {batch_size} batch size, {patience} patience for {epochs} epochs\")\n","              \n","        self.image_data_generator_for_training.fit(train_X)\n","        self.image_data_generator_for_validation.fit(validation_data_X)\n","        modelcheck = ModelCheckpoint(os.path.join('', 'model_and_weights.h5'), monitor = 'val_loss', save_best_only = True, verbose = 0)\n","        earlystopper = EarlyStopping(monitor = 'val_loss', patience = patience, verbose = 1, restore_best_weights = True)\n","        lr_reduce = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_delta = 1e-5, patience = patience, verbose = 1)\n","\n","        self.training_results = self.training_model.fit(\n","            self.image_data_generator_for_training.flow(train_X, train_y, batch_size),\n","            epochs = epochs,\n","            validation_data = self.image_data_generator_for_validation.flow(validation_data_X, validation_data_y, batch_size),\n","            steps_per_epoch = int(len(train_X) / batch_size),\n","            validation_steps = int(len(validation_data_X) / batch_size),\n","            callbacks = [earlystopper, lr_reduce, modelcheck],\n","            class_weight = class_weights\n","        )\n","    \n","    def get_feature_model(self):\n","        #TODO: Improve lazy loading\n","        if self.extracting_features_model == None:\n","            #TODO: Improve layers naming so not to use indexing\n","            #Gets first layer from the classifier model\n","            classifier_layers = self.training_model.layers[1]\n","            classifier_dense_layer = classifier_layers.layers[0]\n","            self.extracting_features_model = Model(\n","                inputs = self.training_model.input,\n","                outputs = classifier_dense_layer.output\n","            )\n","            \n","        return self.extracting_features_model\n","    \n","    def extract_features_with_labels(\n","        self,\n","        train_X,\n","        train_Y,\n","        batch_size\n","    ):\n","        return self.extract_features(train_X, batch_size), np.argmax(train_Y.values, axis = 1)\n","    \n","    def extract_features(\n","        self,\n","        train_X,\n","        batch_size\n","    ):\n","        self.image_data_generator_for_prediction.fit(train_X)\n","        features = self.get_feature_model().predict(self.image_data_generator_for_prediction.flow(train_X, batch_size = batch_size, shuffle = False))\n","        \n","        return features.reshape(features.shape[0], -1)\n","    \n","    def load_model(self, file_path):\n","        self.training_model = load_model(file_path)\n","        self.extracting_features_model = None\n","        \n","    def predict(self, x, batch_size):\n","        self.image_data_generator_for_prediction.fit(x)\n","        return self.training_model.predict(self.image_data_generator_for_prediction.flow(x, batch_size = batch_size, shuffle = False), verbose = 1)\n","    \n","    def predict_species_class(self, x, batch_size):\n","        return np.argmax(self.predict(x, batch_size), axis = 1)\n","    \n","    def predict_species_name(self, x, batch_size):\n","        return [self.species_names[species_class] for species_class in self.predict_species_class(x, batch_size)]\n","\n","    def eval_model(self, test_X, test_Y, field_name = 'species'):\n","        \"\"\"\n","        Model evaluation: plots, classification report\n","        @param training: model training history\n","        @param model: trained model\n","        @param test_X: features \n","        @param test_y: labels\n","        @param field_name: label name to display on plots\n","        \"\"\"        \n","        self.image_data_generator_for_prediction.fit(test_X) \n","        test_pred = self.training_model.predict(self.image_data_generator_for_prediction.flow(test_X, batch_size = batch_size, shuffle = False))\n","        \n","        self.plot_metrics(test_Y, test_pred, field_name)\n","        self.plot_classification_report(self.image_data_generator_for_prediction.flow(test_X, test_Y, batch_size = batch_size, shuffle = False), test_Y, test_pred)\n","        self.plot_confusion_matrix(test_Y, test_pred)\n","    \n","    def configure_fine_tuning(self, layers_to_fine_tune = 10):\n","        print(f\"Fine training model with the following layers:\")\n","        for i, layer in enumerate(self.base_model.layers):\n","            print(f'Layer {i} -> {layer.name}')\n","        \n","        for layer in base_model.layers[: -layers_to_fine_tune]:\n","            layer.trainable = True\n","        \n","        self.training_model.compile(optimizer = self.fine_tune_optimizer, loss = self.loss, metrics = self.metrics)\n","        \n","    def print_layers(self):\n","        for i, layer in enumerate(self.training_model.layers):\n","            print(f'#{i} -> {layer.name}')\n","        \n","    def plot_metrics(self, test_y, test_pred, field_name = 'species'):\n","        fig, axes = plt.subplots(1, 3, figsize = (20, 7))\n","        \n","        self.plot_metric(ConvolutionalTrainingModel.accuracy_metric, axes[0], field_name)\n","        self.plot_metric(ConvolutionalTrainingModel.loss_metric, axes[1], field_name)\n","        self.plot_accuracy_by_species_group(test_y, test_pred, axes[2], field_name)\n","\n","        plt.tight_layout()\n","        plt.show()\n","        \n","    def plot_metric(self, metric, axes, field_name):\n","        axes.plot(self.training_results.history[metric['training']['metric']], label = metric['training']['label'])\n","        axes.plot(self.training_results.history[metric['validation']['metric']], label = metric['validation']['label'])\n","        axes.set_title(f'{field_name} {metric[\"training\"][\"metric\"]}')\n","        axes.set_xlabel('Epoch')\n","        axes.set_ylabel(metric['training']['label'])\n","        axes.legend()\n"," \n","    def plot_accuracy_by_species_group(self, test_y, test_pred, axes, field_name):\n","        acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum() / test_y.sum()\n","        acc_by_subspecies.plot.bar(title = f'Accuracy by {field_name}', ax = axes)\n","        plt.ylabel('Accuracy')\n","    \n","    def plot_classification_report(self, iterator_test_X, test_y, test_pred):\n","        print(\"Classification report\")\n","        test_pred = np.argmax(test_pred, axis = 1)\n","        test_truth = np.argmax(test_y.values, axis = 1)\n","\n","        print(metrics.classification_report(test_truth, test_pred, target_names = test_y.columns, zero_division = True))\n","\n","        test_res = self.training_model.evaluate(iterator_test_X, verbose = 0)\n","        print('Loss function: %s, accuracy:' % test_res[0], test_res[1])\n","        \n","    def plot_confusion_matrix(self, test_Y, test_pred):\n","        cnf_matrix = metrics.confusion_matrix(np.argmax(test_Y.values, axis = 1), np.argmax(test_pred, axis = 1))\n","\n","        abbreviation = ['Bg', 'Ch', 'Cl', 'CC', 'Cw', 'FH', 'LSb', 'M', 'SM', 'SP', 'SfC', 'Sb']\n","        pd.DataFrame({'class': self.species_names, 'abbreviation': abbreviation})\n","\n","        fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n","        ax = sns.heatmap(cnf_matrix, ax = ax, cmap = plt.cm.Greens, annot = True)\n","        ax.set_xticklabels(abbreviation)\n","        ax.set_yticklabels(abbreviation)\n","        plt.title('Confusion matrix of test set')\n","        plt.ylabel('True species')\n","        plt.xlabel('Predicted species')\n","        \n","        plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"xqK4KaHAQsax","trusted":true},"outputs":[],"source":["def split(data):\n","    train_data, validation_data = train_test_split(data, test_size = 0.2, shuffle = True)\n","    validation_data, test_data = train_test_split(validation_data, test_size = 0.5, shuffle = True)\n","\n","    return (train_data, validation_data, test_data)\n","        \n","def create_FCN_classifier_global_max_pooling(batch_normalization = False, dropout = False, kernel_regularizer = None):\n","    model = Sequential(name = 'GlobalMaxPooling-FCN-classifier')\n","    model.add(GlobalMaxPooling2D())\n","    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense1\", activation = 'relu'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense2\", activation = 'relu'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(number_output_classes, activation = 'softmax', name = 'predictions'))\n","        \n","    return model\n","\n","def create_FCN_classifier_global_average_pooling(batch_normalization = False, dropout = False, kernel_regularizer = None):\n","    model = Sequential(name = 'GlobalAveragePooling-FCN-classifier')\n","    model.add(GlobalAveragePooling2D())\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense1\", activation = 'relu'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(number_output_classes, activation = 'softmax', name = 'predictions'))\n","        \n","    return model\n","\n","def create_global_average_pooling():\n","    model = Sequential(name = 'GlobalAveragePooling')\n","    model.add(GlobalAveragePooling2D())\n","    model.add(Dense(1024, kernel_regularizer = None, name = \"dense1\", activation = 'relu'))\n","        \n","    return model\n","    \n","def create_FCN_classifier_with_flatten(batch_normalization = False, dropout = False, kernel_regularizer = None):\n","    model = Sequential(name = 'Flatten-FCN-classifier')\n","    model.add(Flatten())\n","    model.add(Dense(1024, activation = \"relu\", kernel_regularizer = kernel_regularizer, name='dense1'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(number_output_classes, activation = \"softmax\", name='predictions'))\n","    \n","    return model\n","    \n","def create_inceptionV3_model():\n","    base_model = inception_v3.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_vgg16_model():\n","    base_model = vgg16.VGG16(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","     \n","    return base_model\n","\n","def create_vgg19_model():\n","    base_model = vgg19.VGG19(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","     \n","    return base_model\n","\n","def create_resnetv2_model():\n","    base_model = resnet_v2.ResNet50V2(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_inceptionResNetV2_model():\n","    base_model = inception_resnet_v2.InceptionResNetV2(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_xception_model():\n","    base_model = xception.Xception(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_densenet201_model():\n","    base_model = densenet.DenseNet201(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n"," \n","    return base_model\n","\n","def create_mobilenet_v2_model():\n","    base_model = mobilenet_v2.MobileNetV2(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n"," \n","    return base_model\n","\n","def create_nasnet_mobile_model():\n","    base_model = nasnet.NASNetMobile(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n"," \n","    return base_model\n","\n","def create_simple_convolutional_base(batch_normalization = False):\n","    model = Sequential(name = 'Three-Layer-CNN-base')\n","    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), name=\"conv_layer_1.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', name=\"conv_layer_1.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))\n","    \n","    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_2.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_2.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))\n","        \n","    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_3.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_3.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["default_classifier = lambda: create_FCN_classifier_global_average_pooling(batch_normalization = True, dropout = True, kernel_regularizer = l2(0.01))\n","default_image_transformation = segment_plant\n","default_layers_to_fine_tune = 5\n","TRAINING_MODELS_CONFIGURATION = {\n","    ConvolutionalModel.CUSTOM_CNN: {\n","        'preprocessing_function': None,\n","        'rescale': 1. / 255,\n","        'base_model': create_simple_convolutional_base,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': None\n","    },\n","    ConvolutionalModel.RESNET_V2: {\n","        'preprocessing_function': resnet_v2.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_resnetv2_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.INCEPTION_V3: {\n","        'preprocessing_function': inception_v3.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_inceptionV3_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.XCEPTION: {\n","        'preprocessing_function': xception.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_xception_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.INCEPTION_RESNET_V2: {\n","        'preprocessing_function': inception_resnet_v2.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_inceptionResNetV2_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.VGG16: {\n","        'preprocessing_function': vgg16.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_vgg16_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.VGG19: {\n","        'preprocessing_function': vgg19.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_vgg19_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.MOBILE_NET_V2: {\n","        'preprocessing_function': mobilenet_v2.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_mobilenet_v2_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    ConvolutionalModel.NASNET_MOBILE: {\n","        'preprocessing_function': nasnet.preprocess_input,\n","        'rescale': None,\n","        'base_model': create_nasnet_mobile_model,\n","        'classifier': default_classifier,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    },\n","    #TODO: Improve parameters that are not needed\n","    ConvolutionalModel.LOAD_MODEL: {\n","        'preprocessing_function': LOAD_MODEL_PREPROCESS_FUNCTION,\n","        'rescale': None,\n","        'base_model': lambda: None,\n","        'classifier': lambda: None,\n","        'image_transformation': default_image_transformation,\n","        'layers_to_fine_tune': default_layers_to_fine_tune\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"pHowp1l4Qsaz","outputId":"8862f6e0-148d-4a6e-dc75-35ea946fa98b","trusted":true},"outputs":[],"source":["image_transformation = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['image_transformation']\n","\n","train_data, validation_data, test_data = split(all_train_df)\n","\n","train_X, train_Y = load_species_images(train_data, IMAGE_WIDTH, IMAGE_HEIGHT, image_transformation)\n","validation_X, validation_Y = load_species_images(validation_data, IMAGE_WIDTH, IMAGE_HEIGHT, image_transformation)\n","test_X, test_Y = load_species_images(test_data, IMAGE_WIDTH, IMAGE_HEIGHT, image_transformation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"7FBc5BUVQsa1","trusted":true},"outputs":[],"source":["preprocessing_function = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['preprocessing_function']\n","rescale = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['rescale']\n","base_model = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['base_model']()\n","classifier_model = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['classifier']()\n","\n","image_data_generator_for_training = create_image_data_generator(\n","    rotation_range,\n","    zoom_range,\n","    width_shift_range,\n","    height_shift_range,\n","    horizontal_flip,\n","    vertical_flip,\n","    rescale,\n","    preprocessing_function\n",")\n","\n","image_data_generator_for_validation = create_image_data_generator(\n","    rescale = rescale,\n","    preprocessing_function = preprocessing_function\n",")\n","\n","image_data_generator_for_prediction = create_image_data_generator(\n","    rescale = rescale,\n","    preprocessing_function = preprocessing_function\n",")\n","\n","model = ConvolutionalTrainingModel(\n","    image_data_generator_for_training,\n","    image_data_generator_for_validation,\n","    image_data_generator_for_prediction,\n","    base_model,\n","    classifier_model\n",")\n","classifier = model\n","\n","if CONVOLUTIONAL_MODEL == ConvolutionalModel.LOAD_MODEL:\n","    model.load_model(CONVOLUTIONAL_MODEL_WEIGHTS_PATH)\n","    print(f'Model {CONVOLUTIONAL_MODEL_WEIGHTS_PATH} loaded')\n","    model.print_layers()"]},{"cell_type":"markdown","metadata":{},"source":["For hyperparameter optimization there are several methods with [scikit-learn](https://scikit-learn.org/stable/) and [scikit-optimize](https://scikit-optimize.github.io/stable/):\n","- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n","- [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n","- [BayesSearchCV](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html)\n","\n","As mention in its website:\n","> Search for parameters of machine learning models that result in best cross-validation performance is necessary in almost all practical cases to get a model with best generalization estimate. A standard approach in scikit-learn is using sklearn.model_selection.GridSearchCV class, which takes a set of values for every parameter to try, and simply enumerates all combinations of parameter values. The complexity of such search grows exponentially with the addition of new parameters. A more scalable approach is using sklearn.model_selection.RandomizedSearchCV, which however does not take advantage of the structure of a search space.\n","> \n","> Scikit-optimize provides a drop-in replacement for sklearn.model_selection.GridSearchCV, which utilizes Bayesian Optimization where a predictive model referred to as “surrogate” is used to model the search space and utilized to arrive at good parameter values combination as soon as possible."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ClassifierTrainingModel:\n","    n_splits = 3\n","    \n","    def __init__(self, estimator, search_spaces, fit_params, scoring, iterations, convolutional_model):\n","        self.bayes_search = BayesSearchCV(\n","            estimator = estimator,\n","            search_spaces = search_spaces,\n","            scoring = scoring,\n","            cv = StratifiedKFold(\n","                n_splits = ClassifierTrainingModel.n_splits,\n","                shuffle = True,\n","                random_state = 42\n","            ),\n","            fit_params = fit_params,\n","            n_jobs = 1,\n","            n_iter = iterations,   \n","            verbose = 1,\n","            refit = True,\n","            random_state = 42\n","        )\n","        self.convolutional_model = convolutional_model\n","        self.species_names = get_species_groups()\n","    \n","    def fit(self, features, labels):\n","        return self.bayes_search.fit(features, labels, callback = self.print_optimization_status)\n","    \n","    def score(self, features, labels):\n","        return self.bayes_search.score(features, labels)\n","    \n","    def predict_species_class(self, images, batch_size):\n","        features = self.convolutional_model.extract_features(images, batch_size)\n","        return self.bayes_search.predict(features)\n","    \n","    def predict_species_name(self, images, batch_size):\n","        return [self.species_names[species_class] for species_class in self.predict_species_class(images, batch_size)]\n","        \n","    def print_optimization_status(self, results):\n","        \"\"\"Status callback during bayesian hyperparameter search\"\"\"\n","        all_models = pd.DataFrame(self.bayes_search.cv_results_)    \n","\n","        # Get current parameters and the best parameters    \n","        best_params = pd.Series(self.bayes_search.best_params_)\n","        print(f'Model #{len(all_models)}\\nBest score: {np.round(self.bayes_search.best_score_, 4)}\\nBest params: {self.bayes_search.best_params_}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if CONVOLUTIONAL_MODEL != ConvolutionalModel.LOAD_MODEL:\n","    model.train_model(\n","        train_X,\n","        train_Y,\n","        validation_X,\n","        validation_Y,\n","        batch_size,\n","        epochs,\n","        patience,\n","        class_weights\n","    )\n","    \n","    model.eval_model(test_X, test_Y, \"species\")\n","    \n","    layers_to_fine_tune = TRAINING_MODELS_CONFIGURATION[CONVOLUTIONAL_MODEL]['layers_to_fine_tune']\n","    epochs_to_fine_tune = epochs\n","    if layers_to_fine_tune != None:\n","        model.configure_fine_tuning(layers_to_fine_tune)\n","        model.train_model(\n","            train_X,\n","            train_Y,\n","            validation_X,\n","            validation_Y,\n","            batch_size,\n","            epochs_to_fine_tune,\n","            patience,\n","            class_weights\n","        )\n","        \n","        model.eval_model(test_X, test_Y, \"species\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f'Available scoring metrics {metrics.SCORERS.keys()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if CLASSIFIER_MODEL != ClassifierModel.FCN:\n","    features_train, labels_train = model.extract_features_with_labels(train_X, train_Y, batch_size)\n","    features_validation, labels_validation = model.extract_features_with_labels(validation_X, validation_Y, batch_size)\n","    features_test, labels_test = model.extract_features_with_labels(test_X, test_Y, batch_size)\n","\n","    \n","    default_scoring = 'balanced_accuracy'\n","    default_iterations = 5\n","    CLASSIFIER_MODELS_CONFIGURATION = {\n","        ClassifierModel.XGBoost: {\n","            'estimator': xgb.XGBClassifier(\n","                objective = 'multi:softmax',\n","                num_classes = 12,\n","                verbosity = 1\n","            ),\n","            'search_spaces':{\n","                'learning_rate': (0.01, 1.0, 'log-uniform'),\n","                'min_child_weight': (0, 10),\n","                'max_depth': (0, 50),\n","                'max_delta_step': (0, 20),\n","                'subsample': (0.01, 1.0, 'uniform'),\n","                'colsample_bytree': (0.01, 1.0, 'uniform'),\n","                'colsample_bylevel': (0.01, 1.0, 'uniform'),\n","                'reg_lambda': (1e-9, 1000, 'log-uniform'),\n","                'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n","                'gamma': (1e-9, 0.5, 'log-uniform'),\n","                'min_child_weight': (0, 5),\n","                'n_estimators': (50, 100),\n","                'scale_pos_weight': (1e-6, 500, 'log-uniform')\n","            },\n","            'fit_params':{\n","                'eval_set': [(features_validation, labels_validation)],\n","                'eval_metric': 'mlogloss',\n","                'early_stopping_rounds': patience,\n","                'verbose': False\n","            },\n","            'scoring': default_scoring,\n","            'iterations': default_iterations\n","        },\n","        ClassifierModel.LIGHT_GBM: {\n","            'estimator': lgb.LGBMClassifier(\n","                boosting_type = 'gbdt',\n","                n_jobs = -1,\n","                verbose = 1,\n","                objective = 'multiclass',\n","                class_weight = 'balanced',\n","            ),\n","            'search_spaces':{\n","                'learning_rate': (0.01, 1.0, 'log-uniform'),\n","                'num_leaves': (2, 500),\n","                'max_depth': (0, 500),\n","                'min_child_samples': (0, 200),\n","                'max_bin': (100, 100000),\n","                'subsample': (0.01, 1.0, 'uniform'),\n","                'subsample_freq': (0, 10),\n","                'colsample_bytree': (0.01, 1.0, 'uniform'),\n","                'min_child_weight': (0, 10),\n","                'subsample_for_bin': (100000, 500000),\n","                'reg_lambda': (1e-9, 1000, 'log-uniform'),\n","                'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n","                'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n","                'n_estimators': (10, 10000)\n","            },\n","            'fit_params':{\n","                'eval_set': [(features_validation, labels_validation)],\n","                'eval_metric': 'multi_logloss',\n","                'early_stopping_rounds': patience,\n","                'verbose': False\n","            },\n","            'scoring': default_scoring,\n","            'iterations': default_iterations\n","        },\n","        ClassifierModel.SVC: {\n","            'estimator': SVC(\n","                class_weight = 'balanced',\n","                verbose = True,\n","                max_iter = -1\n","            ),\n","            'search_spaces':{\n","                'C': Real(1e-6, 1e+6, prior='log-uniform'),\n","                'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n","                'degree': Integer(1,8),\n","                'kernel': Categorical(['linear', 'poly', 'rbf'])\n","            },\n","            'fit_params': None,       \n","            'scoring': default_scoring,\n","            'iterations': default_iterations\n","        },\n","        ClassifierModel.BEST_MODEL_SVC: {\n","            'estimator': SVC(\n","                class_weight = 'balanced',\n","                verbose = True,\n","                max_iter = -1\n","            ),\n","            'search_spaces':{\n","                'C': [0.083],\n","                'gamma': [3.39],\n","                'degree': [6],\n","                'kernel': ['linear']\n","            },\n","            'fit_params': None,\n","            'scoring': default_scoring,\n","            'iterations': 1\n","        },\n","        ClassifierModel.BEST_MODEL_XGBoost: {\n","            'estimator': xgb.XGBClassifier(\n","                objective = 'multi:softmax',\n","                num_classes = 12,\n","                verbosity = 1\n","            ),\n","            'search_spaces':{\n","                'colsample_bylevel': [0.8142720284737898],\n","                'colsample_bytree': [0.1801528457825951],\n","                'gamma': [0.00015936523535755285],\n","                'learning_rate': [0.4032083917998946],\n","                'max_delta_step': [10],\n","                'max_depth': [5],\n","                'min_child_weight': [4],\n","                'n_estimators': [94],\n","                'reg_alpha': [0.1611980387486336],\n","                'reg_lambda': [387],\n","                'scale_pos_weight': [171],\n","                'subsample': [0.8391548832503206]\n","            },\n","            'fit_params':{\n","                'eval_set': [(features_validation, labels_validation)],\n","                'eval_metric': 'mlogloss',\n","                'early_stopping_rounds': patience,\n","                'verbose': False\n","            },\n","            'scoring': default_scoring,\n","            'iterations': 1\n","        }\n","    }\n","    \n","    classifier_configuration = CLASSIFIER_MODELS_CONFIGURATION[CLASSIFIER_MODEL]\n","    classifier = ClassifierTrainingModel(\n","        classifier_configuration['estimator'],\n","        classifier_configuration['search_spaces'],\n","        classifier_configuration['fit_params'],\n","        classifier_configuration['scoring'],\n","        classifier_configuration['iterations'],\n","        model\n","    )\n","    \n","    classifier.fit(features_train, labels_train)\n","    \n","    print(f\"Training score: {classifier.score(features_train, labels_train)}\")\n","    print(f\"Validation score: {classifier.score(features_validation, labels_validation)}\")\n","    print(f\"Test score: {classifier.score(features_test, labels_test)}\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BY_-wQOWQsa7"},"source":["# 6. Generate prediction file\n","This section generates the prediction file to upload for Kaggle."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"oxAeQ9TOQsa8","trusted":true},"outputs":[],"source":["def load_test():\n","    test = []\n","    files = os.listdir(TEST_DATASET_FOLDER)\n","    for file in tqdm(files, total = len(files)):\n","        test.append([f'{TEST_DATASET_FOLDER}/{file}', file])\n","    \n","    test_df = pd.DataFrame(test, columns=['filepath', 'file'])\n","    \n","    return test_df\n","\n","def generate_prediction_file(classifier, image_width, image_height, preprocess_function = segment_plant):\n","    test_df = load_test()\n","    test_images = load_images(test_df['filepath'], image_width, image_height, preprocess_function)\n","    \n","    species_name_predictions = classifier.predict_species_name(np.stack(test_images), batch_size)\n","    file_output = np.column_stack((test_df['file'], species_name_predictions))\n","    \n","    file_df = pd.DataFrame(file_output, columns = ['file', 'species'])\n","    file_df.to_csv(\"submission.csv\", index = False, index_label = False)\n","    \n","    return file_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"7AYqIdnVQsa-","trusted":true},"outputs":[],"source":["file_df = generate_prediction_file(classifier, IMAGE_WIDTH, IMAGE_HEIGHT)\n","file_df.head(10)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ojBjaPxMQsbA"},"source":["## Learnings\n","\n","1. Skimage could not handle the images loading with 16 GB RAM limit so OpenCV was used instead.\n","2. t-SNE improve after applying segmentation and masking over the images, but still cannot cluster correctly on lower dimensions.\n","3. Tensorflow and keras libraries were upgraded after migrating from Kaggle to Google Colab. Kaggle has a retriction of 30 hours and 2 GPUs. There are specific configuration for Google Colab and Kaggle.\n","4. Don't forget to use preprocess_input specific for each pre-trained model even when loading the HDF5 models.\n","5. Using configuration dictionaries helps the extensibility of the model's configuration code.\n","6. Fine tuning the last layers of the convolutional model can make a big difference for transfer learning."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fyurinPoQsbB"},"source":["# Links\n","\n","This are some papers and links use during the exercise resolution:\n","\n","- [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239.pdf)\n","- [A New Design Based-SVM of the CNN Classifier Architecture with Dropout for Offline Arabic Handwritten Recognition](https://www.sciencedirect.com/science/article/pii/S1877050916309991)\n","- [Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)\n","- [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)\n","- [Regularization and Optimization strategies in Deep Convolutional Neural Network](https://arxiv.org/pdf/1712.04711.pdf)\n","- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","- [Revisiting small batch training for deep neural networks](https://arxiv.org/pdf/1804.07612.pdf)\n","- [Pre-trained Xception](https://www.kaggle.com/liorbrag/keras-pretrained-xception-0-96977-on-public)"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Plant Seedlings Classification with xception.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}