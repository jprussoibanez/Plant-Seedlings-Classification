{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The project\nThe problem we want to analyze is the [Kaggle plant seedlings classification](https://www.kaggle.com/c/plant-seedlings-classification).\n\n##  Competition goal\nThe goal is to differentiate a weed from a crop seedling in order to perform site-specific weed control.\n\n## Dataset\nThe [database of images](https://arxiv.org/abs/1711.05458) has approximately 960 unique plants belonging to 12 species at several growth stages. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\n\n## Kernel structure\nThe following is a summary of the kernel main structure.\n\n### 1. Kagglers solutions and discussions\n- Review other kagglers kernels to look into possible solutions.\n- Review the competition discussion forum for better understanding of the challenges and general ideas.\n\n### 2. Libraries and settings\n- This section has all available settings to configure the training and model configuration.\n\n### 3. Data analysis\n- Main data exploration and analysis to determine the best model to solve the challenge.\n- Use of descriptive analysis to determine dataset distribution.\n- Use of t-SNE to reduce dimensionality for data visualization.\n\n### 4. Pre-processing\n- Use class weights to balance the dataset.\n- Image segmentation to remove image soil background.\n- Data augmentation to increase the dataset.\n\n### 5. Processing\n- Use of transfer learning with different pre-trained networks Resnet50 and InceptionV3. Other networks can be easily added.\n- Use of custom simple CNN. This configuration can be improve by using hyperparameters optimization like [hyperopt](https://github.com/hyperopt/hyperopt).\n- Use of a simple FNN as a classifier. This can be improved by using other classifiers like SVM, XGBoost, etc.\n\n### 6. Generate prediction file\n- Generate prediction file with Kaggle competition format."},{"metadata":{},"cell_type":"markdown","source":"# 1. Kagglers solutions and discussions\nFirst step is to review the discussion forum and kernels from other kagglers in solving this competition to discover solutions and approaches to try and validate.\n\n## Discussion forum\nThe following are the interesting discussions forum:\n1. [Can Top People in the LeaderBoard share their apporoach?](https://www.kaggle.com/c/plant-seedlings-classification/discussion/53021). This links to [Kumar Sridhar Medium Post](https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86) explaining how he reach the 5th position on the leaderboard.\n2. [Plant Segmentation does not effect (that much) accuracy](https://www.kaggle.com/c/plant-seedlings-classification/discussion/45271). This discussion shows finally that segmentation do have an increase on acurracy [3-8]% and links to a [kernel](https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision) with the solution.\n3. [Some Tips to Improve accuracy](https://www.kaggle.com/c/plant-seedlings-classification/discussion/46699). This a discussion on image size and pre-trained models to improve accuracy. The recommendation is to use image sizes between 300-400pxs and simple pre-trained models like ResNet or Inception. [Links](https://github.com/GodsDusk/Plant-Seedlings-Classification) to another solution from Kaggler positioned 47th on the leaderboard. Also explains how to use [pre-trained models with different input image sizes](https://stackoverflow.com/questions/44161967/keras-vggnet-pretrained-model-variable-sized-input). Another [discussion](https://www.kaggle.com/c/plant-seedlings-classification/discussion/45206) confirms better accuracy between 200-400pxs on image size.\n4. [image background generalization](https://www.kaggle.com/c/plant-seedlings-classification/discussion/50323). This links to a kernel for [image segmentation and background removal](https://www.kaggle.com/ianchute/background-removal-cieluv-color-thresholding)\n5. [Test images from the wild](https://www.kaggle.com/c/plant-seedlings-classification/discussion/44490). Author from the competition dataset shares the link to the [dataset webpage](https://vision.eng.au.dk/plant-seedlings-dataset/).\n\n## Kagglers Kernels\nThe following are interesting sample kagglers kernels.\n1. [Keras simple model (0.97103 Best Public Score)](https://www.kaggle.com/miklgr500/keras-simple-model-0-97103-best-public-score) This shows a simple custom deep network can achieve high scores.\n2. [Seedlings - Pretrained keras models](https://www.kaggle.com/gaborfodor/seedlings-pretrained-keras-models). Kernel showing transfer learning with a Xception pre-trained model (around 83.6% accuracy).\n3. [Plants PCA & t-SNE ( w/ image scatter plot)](https://www.kaggle.com/gaborvecsei/plants-t-sne) Nice data visualization with t-SNE.\n4. [Plants Xception 90.06% Test Accuracy](https://www.kaggle.com/raoulma/plants-xception-90-06-test-accuracy). Kernel with transfer learning on Xception and different classification models with Logistic Regression, random forest and fully connect neural network.\n5. [CNN + SVM + XGBoost](https://www.kaggle.com/matrixb/cnn-svm-xgboost) Combine other classifiers (SVM, XGBoost, Logistic regression, etc.) with the convolutional layers\n\n## Conclusions\nFrom the discussion forums and kernels we can conclude that:\n1. There is good high scores with simple pre-trained models like Xception or ResNet (they recommend not to use more complex models like DenseNet or ResNext) and also on custom simple deep learning models. We should try both approaches for comparison and use it to benchmark our results. \n2. For the classification layer kagglers try different classifiers like XGBoost, SVM, Logistic Regression and Random Forest with different results. We should try some of this approaches to compare results.\n3. Segmentation was shown to improve performance. As images are specially photograph for training purposes so it seems there is no need to crop by applying [bounding boxes](https://www.kaggle.com/martinpiotte/bounding-box-data-for-the-whale-flukes) like in this [whale competition](https://www.kaggle.com/c/humpback-whale-identification).\n4. The datasets do have a small ammount of images and have imbalanced classes, so we should use data augmentation and other techniques to balance the dataset for better results.\n\nAll this assumptions will be validated through our own analyzes of the data and results."},{"metadata":{},"cell_type":"markdown","source":"# 2. Libraries and settings\n\n## 2.1 Libraries\n\nWe will be using the following main libraries for coding:\n1. [tensorflow](https://www.tensorflow.org/) with [keras](https://keras.io/) for managing the deep learning models.\n2. [sckilit-learn](https://scikit-learn.org/), [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/) with [seaborn](https://seaborn.pydata.org/) for data manipulation, analysis and visualization.\n3. [TQDM](https://github.com/tqdm/tqdm) for progress bar visualization on processing.\n4. Some general libraries for environment management. [os](https://docs.python.org/3/library/os.html)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\n# Keras\nfrom keras.applications import vgg16, inception_v3, resnet50, mobilenet, densenet\nfrom keras.optimizers import SGD, Adam\nfrom keras.regularizers import l1, l2, l1_l2\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, GlobalAveragePooling2D, Input, Activation, BatchNormalization, GlobalMaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n\nimport os\n\n# Helpers\nfrom tqdm import tqdm\nfrom enum import Enum\n\n# Image processing\nimport cv2\n\n# Image plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\n\n# Sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.utils.class_weight import compute_class_weight\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Global variables\n\nThis are the global variables use to setup the configuration for the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global settings\nBASE_DATASET_FOLDER = \"../input/plant-seedlings-classification\"\nTRAIN_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"train\")\nTEST_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"test\")\nIMAGE_WIDTH = 299\nIMAGE_HEIGHT = 299\nIMAGE_CHANNELS = 3\n\nTSNE_VISUALIZATION = False\n\n# Data augmentation settings\nrotation_range = 250      \nzoom_range = 0.5         \nwidth_shift_range = 0.5  \nheight_shift_range = 0.5 \nhorizontal_flip = True   \nvertical_flip = True\n\n# Training settings\nbatch_size = 150\nepochs = 500\nsteps_per_epoch = 150\npatience = 10\n\nclass TrainingNetwork(Enum):\n    CUSTOM_CNN = 1    # Simple custom convolutional network with 3 layers\n    RESNET_50 = 2     # Pre-trained Resnet50\n    INCEPTION_V3 = 3  # Pre-trained InceptionV3\n\n# Choose model to train\nNETWORK_TO_TRAIN = TrainingNetwork.CUSTOM_CNN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data analysis\n\nThe first and the most important task in solving a problem with Machine Learning is to analyze the dataset before proceeding with any algorithms. This is important in order to understand the complexity of the dataset which will eventually help in designing the algorithm."},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Load data and images\n\nThe first step is to load the images and data from the datasets. For this we'll be using panda library and dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_species_groups():\n    \"\"\" \n    Get the species group from the folders' name on the dataset. \n  \n    @Parameters:\n        train_dataset_folder (string): Folder path to the trained dataset.\n  \n    @Returns: \n        array: Array of species groups to train.\n    \"\"\"\n    return ['Loose Silky-bent', \n            'Shepherds Purse', \n            'Fat Hen', \n            'Charlock', \n            'Sugar beet', \n            'Maize', \n            'Common wheat', \n            'Cleavers', \n            'Black-grass', \n            'Small-flowered Cranesbill', \n            'Scentless Mayweed', \n            'Common Chickweed']\n\ndef read_train_data(species_groups):\n    \"\"\" \n    Read the train species data from the datasets folder. \n  \n    @Parameters:\n        species_groups (array): Array of species groups to train.\n  \n    @Returns: \n        dataframe: Dataframe with the filepath, filename and species group category for each species image.\n    \"\"\"\n    train = []\n    for _, species in tqdm(enumerate(species_groups), total=len(species_groups)):\n        for file in os.listdir(os.path.join(TRAIN_DATASET_FOLDER, species)):\n            train.append([f'{TRAIN_DATASET_FOLDER}/{species}/{file}', file, species])\n    \n    train_df = pd.DataFrame(train, columns=['filepath', 'file', 'species'])\n    \n    return train_df\n\ndef read_image(image_file_path, image_width, image_height):\n    \"\"\" \n    Read and transform image according to widht, height and channels.\n  \n    @Parameters:\n        image_file_path(string): Image file path to read.\n        image_width (int): Image width to resize.\n        image_height (int): Image height to resize.\n  \n    @Returns: \n        array: Array of loaded images resized to image_width and image_height.\n    \"\"\"\n    image = cv2.imread(image_file_path, cv2.IMREAD_COLOR)\n    image = cv2.resize(image, (image_width, image_height))\n    \n    return image\n\ndef load_images(filepaths, image_width, image_height, preprocess_function = lambda x: x):\n    return filepaths.progress_apply(\n        lambda image_file_path: preprocess_function(read_image(image_file_path, image_width, image_height))\n    )\n    \ndef load_species_images(species_data, image_width, image_height, preprocess_function = lambda x: x):\n    \"\"\" \n    Load images to input features on neural network model and do one hot encode for labels.\n  \n    @Parameters:\n        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n        image_width (int): Image width to resize.\n        image_height (int): Image height to resize.\n       \n    @Returns: \n        species_X (array): Array of images on the data.\n        species_Y (array): One hot encode for labels.\n    \"\"\" \n    species_X = load_images(species_data['filepath'],image_width, image_height, preprocess_function)\n    species_Y = pd.get_dummies(species_data['species'], drop_first = False)\n    \n    return np.stack(species_X), species_Y\n\ndef show_grid_multiple_sample(species_data, species_groups):\n    \"\"\" \n    Plot sample images from each species group.\n  \n    @Parameters:\n        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n        species_groups (array): Array with species group.\n       \n    @Returns: None\n    Plots grid with number_of_samples images with image_size for each species_group\n    \"\"\"\n    number_of_samples = 8\n    image_size = 100\n    \n    number_of_species = len(species_groups) \n    fig, axes = plt.subplots(nrows = number_of_species, ncols = number_of_samples + 1, figsize = (20, 28), gridspec_kw = {'wspace': 0.05, 'hspace': 0})\n \n    for species_id, species in enumerate(species_groups):\n        samples_species_filepath = species_data[species_data['species'] == species]['filepath'].sample(number_of_samples)\n        ax = axes[species_id, 0]\n        ax.axis('off')\n        ax.text(0.5, 0.5, species, horizontalalignment = 'center', verticalalignment = 'center', fontsize = 10, transform = ax.transAxes)\n        for sample_id, sample_species_filepath in enumerate(samples_species_filepath):\n            image = read_image(sample_species_filepath, image_size, image_size)\n            ax = axes[species_id, sample_id + 1]\n            ax.axis('off')\n            ax.imshow(image)\n            \n    plt.show()\n\ndef show_grid_one_sample(species_data, species_groups):\n    \"\"\" \n    Plot one sample image from each species group.\n  \n    @Parameters:\n        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n        species_groups (array): Array with species group.\n       \n    @Returns: None\n    Plots one sample image with image_size for each species_group\n    \"\"\"\n    image_size = 100\n    number_of_species = len(species_groups)\n    \n    fig, axes = plt.subplots(nrows = 2, ncols = int(number_of_species / 2), figsize = (20, 8), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n    axes = axes.flatten()\n    \n    for group_id, (group_name, group) in enumerate(species_data.groupby('species')):\n        image = read_image(group.sample(1).iloc[0]['filepath'], image_size, image_size)\n        axes[group_id].axis('off')\n        axes[group_id].set_title(group_name)\n        axes[group_id].imshow(image)\n            \n    plt.show()\n    \ndef show_species_distribution(species_data):\n    \"\"\" \n    Plot species distribution.\n  \n    @Parameters:\n        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n       \n    @Returns: None\n    Plots species distribution on an histogram and pie chart.\n    \"\"\"\n    species_groups = species_data['species'].unique()\n    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n    colors_palette = sns.husl_palette(len(species_groups), h = 0.01, l = 0.6, s = .2)\n    explode_chart = np.full(len(species_groups), 0.1)\n    \n    species_data.groupby('species').size().plot.bar(\n        subplots = True, ax = axes[0], stacked = True, title = 'Species distribution', label = \"\", color = colors_palette\n    )\n     \n    species_data['species'].value_counts().plot.pie(\n        subplots = True, ax = axes[1], autopct = '%.2f', title = 'Species distribution', label = \"\", colors = colors_palette, explode = explode_chart\n    )\n    \n    plt.show()\n    \nclass TSNE_dimension(Enum):\n    TWO = 1\n    THREE = 2\n    \ndef apply_PCA(images, n_components = 100):\n    pca = PCA(n_components = n_components)\n    pca_result = pca.fit_transform(images)\n    \n    return pca_result\n\ndef apply_TSNE(images, n_components = 2, perplexity = 100):\n    tsne = TSNE(n_components = n_components, perplexity = perplexity)\n    tsne_result = tsne.fit_transform(images)\n    \n    return tsne_result\n\ndef plot_TSNE_with_PCA(images, image_labels, dimension = TSNE_dimension.TWO):\n    pca_results = apply_PCA(images)\n    n_components = (2 if dimension == TSNE_dimension.TWO else 3)\n    tsne_result = apply_TSNE(pca_results, n_components)\n    plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO)\n\ndef plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO):\n    df = pd.DataFrame(tsne_result).assign(label = image_labels)\n    plt.figure(figsize=(10,10))\n    if dimension == TSNE_dimension.TWO:\n        sns.scatterplot(x = 0, y = 1, hue = 'label', data = df);\n    else:\n        fig = px.scatter_3d(df, x = 0, y = 1, z = 2, color = 'label')\n        fig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1.1 Load data\n\nLoad data from the datasets folders."},{"metadata":{"trusted":true},"cell_type":"code","source":"species_groups = get_species_groups()\nprint(f\"This are the groups to be loaded and trained: {species_groups}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_df = read_train_data(species_groups)\nall_train_df.sample(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1.2 Descriptive analysis\n\nWe will do a descriptive analysis on the load data as followed:\n1. Show sample images by species group.\n2. Show species group distribution.\n3. Dimensionality reduction through PCA & t-SNE."},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2.1 Sample images by species group\n\nWe can see that it even difficult for an inexperienced human eye to distinguish between the different weeds species groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_grid_multiple_sample(all_train_df, species_groups)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_grid_one_sample(all_train_df, species_groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2.2 Species group distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_species_distribution(all_train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2.3 PCA & TSNE\n\nWe use PCA and t-SNE to apply dimensionality reduction to images on 2D and 3D. There seems not much relationship before applying the pre-processing. We try again after pre-processing with segmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"if TSNE_VISUALIZATION:\n    sample_df = all_train_df.sample(2000)\n    sample_images, _ = load_species_images(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TSNE_VISUALIZATION:\n    flatten_images = [image.flatten() for image in sample_images]\n    pca_results = apply_PCA(flatten_images)\n    tsne_results = apply_TSNE(pca_results)\n    plot_TSNE_results(tsne_results, sample_df['species'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TSNE_VISUALIZATION:\n    tsne_results = apply_TSNE(pca_results, n_components = 3)\n    plot_TSNE_results(tsne_results, sample_df['species'].values, TSNE_dimension.THREE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Conclusion\n\nFrom the descriptive analysis we can conclude:\n- There is an imbalance on the dataset which we should handle to avoid biases.\n- There is not much images for training on deep learning models so we should use data augmentation.\n- All weeds images have a common soil background that we should remove to improve weed recognition.\n- There is no similarity on dimensionality reduction yet before pre-processing."},{"metadata":{},"cell_type":"markdown","source":"# 4. Pre-processing\n\nFrom the analysis conclusions we can prepare our data for preprocessing to tackle each.\n\n1. Balance dataset to avoid biases on imbalance datasets.\n2. Segmentation to remove soil background and focus on weed recognition.\n3. Data augmentation to increase the data available for training."},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Balance dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_class_weights(data_groups) :\n    return dict(enumerate(compute_class_weight(\"balanced\", np.unique(data_groups), data_groups)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = compute_class_weights(all_train_df['species'])\nprint(f\"This are the class weights: {class_weights}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ndef create_image_tranformation(image):\n    masked_image = create_mask_for_plant(image)\n    segmented_image = segment_plant(image)\n    \n    return masked_image, segmented_image\n\ndef load_species_images_with_segmentation(species_data, image_width, image_height):\n    return load_species_images(species_data, image_width, image_height, segment_plant)\n\ndef plot_image_transformation(original_image, masked_image, segmented_image):\n    fig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(10, 10))\n    axs[0].imshow(original_image)\n    axs[0].set_title('Original image')\n    axs[0].axis('off')\n    axs[1].imshow(masked_image)\n    axs[1].axis('off')\n    axs[1].set_title('Masked image')\n    axs[2].imshow(segmented_image)\n    axs[2].set_title('Segmented image')\n    axs[2].axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_image, _ = load_species_images(all_train_df.sample(1), IMAGE_WIDTH, IMAGE_HEIGHT)\nmasked_image, segmented_image = create_image_tranformation(sample_image[0])\nplot_image_transformation(sample_image[0], masked_image, segmented_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying t-SNE again on the same sample images but now with segmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"if TSNE_VISUALIZATION:\n    sample_images, _ = load_species_images_with_segmentation(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)\n    flatten_images = [image.flatten() for image in sample_images]\n    plot_TSNE_with_PCA(flatten_images, sample_df['species'].values, TSNE_dimension.TWO)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Data augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_image_data_generator(\n    rotation_range,  \n    zoom_range,\n    width_shift_range, \n    height_shift_range, \n    horizontal_flip,\n    vertical_flip\n):\n    return ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False,\n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        rotation_range = rotation_range,\n        zoom_range = zoom_range,\n        width_shift_range = width_shift_range,\n        height_shift_range = height_shift_range,\n        horizontal_flip = horizontal_flip,\n        vertical_flip = vertical_flip,\n        rescale = (1 / 255)\n    )\n    \ndef show_augmentation_row(original_image, images_iterator, number_of_augmentations, axes):   \n    axes[0].axis('off')\n    axes[0].set_title('Original image')\n    axes[0].imshow(original_image)\n    \n    for image_number in range(number_of_augmentations):\n        image = images_iterator.next()\n        axes[image_number + 1].axis('off')\n        axes[image_number + 1].set_title('Augmented image')\n        axes[image_number + 1].imshow(image[0], vmin=0, vmax=1)\n\ndef show_grid_with_augmented_images(number_of_samples, number_of_augmentations, image_data_generator):   \n    fig, axes = plt.subplots(nrows = number_of_samples, ncols = number_of_augmentations + 1, figsize = (30, 4 * number_of_samples), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n    fig.suptitle('Sample augmented images', fontsize=16)\n    \n    sample_images, _ = load_species_images_with_segmentation(all_train_df.sample(number_of_samples), IMAGE_WIDTH, IMAGE_HEIGHT)\n    for sample_id, sample_image in enumerate(sample_images):\n        images_iterator = image_data_generator.flow(np.expand_dims(sample_image, axis=0), None, 1)\n        show_augmentation_row(sample_image, images_iterator, number_of_augmentations, axes[sample_id])\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_generator = create_image_data_generator(\n    rotation_range = 250,    \n    zoom_range = 0.2,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip = True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_grid_with_augmented_images(number_of_samples = 5, number_of_augmentations = 7, image_data_generator = image_data_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Processing\nThis are the following architectures use for training:\n\n- Pre-trained models: This models are use for transfer learning by removing the classifier layer and training on specific for the problem. \n- Custom CNN: This is a simple multi-layer CNN with batch normalization and regularization \n\nAlso different metrics are use to validate the model like accuracy, recall, f1-score and the confusion matrix.\n\nThe training set is divided in training (80%), validation (10%) and test sets (10%). An improvement could be to run cross-validation although it takes more time to train as well.\n\nSome improvements not done but could improve the solution performance are:\n\n- Fine tune pre-trained models by unfreezing and training the last layers on the CNN.\n- Use hyperparameters optimization using some library like hyperopt to try different model parameters.\n- Use more classifiers like XGBoost, SVM, etc. instead of the simple FNN.\n- Ensemble different models to improve performance by combining different solution approaches.\n- Use cross-validation to improve the training process although it consumes more time to train as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"number_output_classes = len(get_species_groups())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainingModel:\n    accuracy_metric = {\n        'training': {\n            'metric': 'accuracy',\n            'label': 'Accuracy'\n        },\n        'validation': {\n            'metric': 'val_accuracy',\n            'label': 'Validation accuracy'\n        }\n    }\n    loss_metric = {\n        'training': {\n            'metric': 'loss',\n            'label': 'Loss'\n        },\n        'validation': {\n            'metric': 'val_loss',\n            'label': 'Validation loss'\n        }\n    }\n    \n    def __init__(self, image_data_generator, base_model, classifier_model):\n        self.image_data_generator = image_data_generator\n        self.optimizer = Adam(lr=1e-2)\n        self.loss = 'categorical_crossentropy'\n        self.metrics = [TrainingModel.accuracy_metric['training']['metric']]\n        self.training_results = None\n        self.training_model = self.create_model(base_model, classifier_model)\n        self.species_names = get_species_groups()\n    \n    def __del__(self):\n        del self.image_data_generator\n        del self.training_model\n        del self.training_results\n    \n    def create_model(self, base_model, classifier_model):\n        model = Sequential(name = \"Species-Prediction-CNN\")\n        model.add(base_model)\n        model.add(classifier_model)\n\n        model.compile(optimizer = self.optimizer, loss = self.loss, metrics = self.metrics)  \n        model.summary()\n\n        return model\n\n    def train_model(\n        self,\n        train_X,\n        train_y,\n        validation_data_X,\n        validation_data_y,\n        batch_size,\n        epochs,\n        steps_per_epoch,\n        patience,\n        class_weights\n    ):\n    \n        self.image_data_generator.fit(train_X)\n        earlystopper = EarlyStopping(monitor = 'loss', patience = patience, verbose = 1, restore_best_weights = True)\n        lr_reduce = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_delta = 1e-5, patience = patience, verbose = 1)\n\n        self.training_results = self.training_model.fit_generator(\n            self.image_data_generator.flow(train_X, train_y, batch_size),\n            epochs = epochs,\n            validation_data = [validation_data_X, validation_data_y],\n            steps_per_epoch = steps_per_epoch,\n            callbacks = [earlystopper, lr_reduce],\n            class_weight = class_weights\n        )\n        \n    def predict(self, x):\n        return self.training_model.predict(x, verbose = 1)\n    \n    def predict_species_class(self, x):\n        return np.argmax(self.predict(x), axis = 1)\n    \n    def predict_species_name(self, x):\n        return [self.species_names[species_class] for species_class in self.predict_species_class(x)]\n\n    def eval_model(self, test_X, test_Y, field_name = 'species'):\n        \"\"\"\n        Model evaluation: plots, classification report\n        @param training: model training history\n        @param model: trained model\n        @param test_X: features \n        @param test_y: labels\n        @param field_name: label name to display on plots\n        \"\"\"        \n        test_pred = self.training_model.predict(test_X)\n \n        self.plot_metrics(test_X, test_Y, test_pred, field_name)\n        \n        self.plot_classification_report(test_X, test_Y, test_pred)\n        \n        self.plot_confusion_matrix(test_Y, test_pred)\n    \n    def plot_metrics(self, test_X, test_y, test_pred, field_name = 'species'):\n        fig, axes = plt.subplots(1, 3, figsize = (20, 7))\n        \n        self.plot_metric(TrainingModel.accuracy_metric, axes[0], field_name)\n        self.plot_metric(TrainingModel.loss_metric, axes[1], field_name)\n        self.plot_accuracy_by_species_group(test_X, test_y, test_pred, axes[2], field_name)\n\n        plt.tight_layout()\n        plt.show()\n        \n    def plot_metric(self, metric, axes, field_name):\n        axes.plot(self.training_results.history[metric['training']['metric']], label = metric['training']['label'])\n        axes.plot(self.training_results.history[metric['validation']['metric']], label = metric['validation']['label'])\n        axes.set_title(f'{field_name} {metric[\"training\"][\"metric\"]}')\n        axes.set_xlabel('Epoch')\n        axes.set_ylabel(metric['training']['label'])\n        axes.legend()\n \n    def plot_accuracy_by_species_group(self, test_X, test_y, test_pred, axes, field_name):\n        acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum() / test_y.sum()\n        acc_by_subspecies.plot.bar(title = f'Accuracy by {field_name}', ax = axes)\n        plt.ylabel('Accuracy')\n    \n    def plot_classification_report(self, test_X, test_y, test_pred):\n        print(\"Classification report\")\n        test_pred = np.argmax(test_pred, axis = 1)\n        test_truth = np.argmax(test_y.values, axis = 1)\n\n        print(metrics.classification_report(test_truth, test_pred, target_names = test_y.columns, zero_division = True))\n\n        test_res = self.training_model.evaluate(test_X, test_y.values, verbose = 0)\n        print('Loss function: %s, accuracy:' % test_res[0], test_res[1])\n        \n    def plot_confusion_matrix(self, test_Y, test_pred):\n        cnf_matrix = metrics.confusion_matrix(np.argmax(test_Y.values, axis = 1), np.argmax(test_pred, axis = 1))\n\n        abbreviation = ['Bg', 'Ch', 'Cl', 'CC', 'Cw', 'FH', 'LSb', 'M', 'SM', 'SP', 'SfC', 'Sb']\n        pd.DataFrame({'class': self.species_names, 'abbreviation': abbreviation})\n\n        fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n        ax = sns.heatmap(cnf_matrix, ax = ax, cmap = plt.cm.Greens, annot = True)\n        ax.set_xticklabels(abbreviation)\n        ax.set_yticklabels(abbreviation)\n        plt.title('Confusion matrix of test set')\n        plt.ylabel('True species')\n        plt.xlabel('Predicted species')\n        \n        plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(data):\n    train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 13)\n    test_data, validation_data = train_test_split(test_data, test_size = 0.5, random_state = 13)\n\n    return (train_data, validation_data, test_data)\n        \ndef create_CNN_classifier_global_max_pooling(batch_normalization = False, dropout = False, kernel_regularizer = None):\n    model = Sequential(name = 'GlobalMaxPooling-FNN-classifier')\n    model.add(GlobalMaxPooling2D())\n    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense1\", activation='relu'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense2\", activation='relu'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(number_output_classes, activation = 'softmax', name = 'predictions'))\n        \n    return model\n\ndef create_CNN_classifier_with_flatten(batch_normalization = False, dropout = False, kernel_regularizer = None):\n    model = Sequential(name = 'Flatten-FNN-classifier')\n    model.add(Flatten())\n    model.add(Dense(1024, activation = \"relu\", kernel_regularizer = kernel_regularizer, name='dense1'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(number_output_classes, activation = \"softmax\", name='predictions'))\n    \n    return model\n    \ndef create_inceptionV3_model():\n    base_model = inception_v3.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    return base_model\n\ndef create_vgg16_model():\n    base_model = vgg16.VGG16(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n    for layer in base_model.layers:\n        layer.trainable = False\n     \n    return base_model\n\ndef create_resnet50_model():\n    base_model = resnet50.ResNet50(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    return base_model\n\ndef create_densenet201_model():\n    base_model = densenet.DenseNet201(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n    for layer in base_model.layers:\n        layer.trainable = False\n \n    return base_model\n\ndef create_simple_convolutional_base(batch_normalization = False):\n    model = Sequential(name = 'Three-Layer-CNN-base')\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), name=\"conv_layer_1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size = (2, 2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu', name = \"conv_layer_2\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size = (2, 2), strides =(2, 2)))\n        \n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu', name = \"conv_layer_3\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, validation_data, test_data = split(all_train_df)\n\ntrain_X, train_Y = load_species_images_with_segmentation(train_data, IMAGE_WIDTH, IMAGE_HEIGHT)\nvalidation_X, validation_Y = load_species_images_with_segmentation(validation_data, IMAGE_WIDTH, IMAGE_HEIGHT)\ntest_X, test_Y = load_species_images_with_segmentation(test_data, IMAGE_WIDTH, IMAGE_HEIGHT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_generator = create_image_data_generator(\n    rotation_range,\n    zoom_range,\n    width_shift_range,\n    height_shift_range,\n    horizontal_flip,\n    vertical_flip\n)\n\nif NETWORK_TO_TRAIN == TrainingNetwork.CUSTOM_CNN:\n    base_model = create_simple_convolutional_base(batch_normalization = True)\nif NETWORK_TO_TRAIN == TrainingNetwork.RESNET_50:\n    base_model = create_resnet50_model()\nif NETWORK_TO_TRAIN == TrainingNetwork.INCEPTION_V3:\n    base_model = create_resnet50_model()\n\nclassifier_model = create_CNN_classifier_global_max_pooling(batch_normalization = True, dropout = True, kernel_regularizer = l2(0.01))\nmodel = TrainingModel(image_data_generator, base_model, classifier_model)\nmodel.train_model(\n    train_X,\n    train_Y,\n    validation_X,\n    validation_Y,\n    batch_size,\n    epochs,\n    steps_per_epoch,\n    patience,\n    class_weights\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval_model(test_X, test_Y, \"species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Generate prediction file\nThis section generates the prediction file to upload for Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_test():\n    test = []\n    files = os.listdir(TEST_DATASET_FOLDER)\n    for file in tqdm(files, total = len(files)):\n        test.append([f'{TEST_DATASET_FOLDER}/{file}', file])\n    \n    test_df = pd.DataFrame(test, columns=['filepath', 'file'])\n    \n    return test_df\n\ndef generate_prediction_file(model, image_width, image_height, preprocess_function = segment_plant):\n    test_df = load_test()\n    test_images = load_images(test_df['filepath'], image_width, image_height, preprocess_function)\n    \n    species_name_predictions = model.predict_species_name(np.stack(test_images))\n    file_output = np.column_stack((test_df['file'], species_name_predictions))\n    \n    file_df = pd.DataFrame(file_output, columns = ['file', 'species'])\n    file_df.to_csv(\"prediction_file.csv\", index = False, index_label = False)\n    \n    return file_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_df = generate_prediction_file(model, IMAGE_WIDTH, IMAGE_HEIGHT)\nfile_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learnings\n\n1. Image library: At the beginning I use skimage instead of cv2 for loading images and was not able to load the complete set of images into 16 GB RAM.\n2. t-SNE improve after applying segmentation and masking over the images, but still cannot cluster correctly on lower dimensions."},{"metadata":{},"cell_type":"markdown","source":"# Links\n- [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239.pdf)\n- [A New Design Based-SVM of the CNN Classifier Architecture with Dropout for Offline Arabic Handwritten Recognition](https://www.sciencedirect.com/science/article/pii/S1877050916309991)\n- [Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)\n- [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)\n- [Regularization and Optimization strategies in Deep Convolutional Neural Network](https://arxiv.org/pdf/1712.04711.pdf)\n- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n- [Revisiting small batch training for deep neural networks](https://arxiv.org/pdf/1804.07612.pdf)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}