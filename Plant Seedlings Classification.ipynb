{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Plant Seedlings Classification.ipynb","provenance":[{"file_id":"https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/Plant_Seedlings_Classification.ipynb","timestamp":1582636019657}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/jprussoibanez/plant-seedlings-classification/blob/master/Plant%20Seedlings%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bFaXLFpsVqGG"},"source":["### IMPORTANT: Change BASE_DATASET_FOLDER with the root dataset folder with competition images."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"_2UIXqgKVgH3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change BASE_DATASET_FOLDER according to folder with dataset\n","BASE_DATASET_FOLDER = \"/content/drive/My Drive/Colab Notebooks/Tryolabs-CV-Interview/data/\""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KvS6f_7BQsZq"},"source":["# The project\n","The problem to analyze is the [Kaggle plant seedlings classification](https://www.kaggle.com/c/plant-seedlings-classification).\n","\n","##  Competition goal\n","The goal is to differentiate a weed from a crop seedling in order to perform site-specific weed control.\n","\n","## Dataset\n","The [database of images](https://arxiv.org/abs/1711.05458) has approximately 960 unique plants belonging to 12 species at several growth stages. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\n","\n","## Kernel structure\n","The following is a summary of the kernel main structure.\n","\n","### 1. Kagglers challenges and discussions\n","- Review other kagglers kernels to better understand the competition challenges.\n","- Review the competition discussion forum for interesting conversation threads.\n","- Generate insights from the information gathering.\n","\n","For further information please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/kagglers_discussions.md).\n","\n","### 2. Libraries and settings\n","- This section has available settings to configure the model and its training parameters.\n","\n","For further information please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/settings.md).\n","\n","### 3. Data analysis\n","- Data exploration and descriptive analysis to determine dataset shape and distribution.\n","- Use of t-SNE to reduce dimensionality for data visualization.\n","\n","### 4. Pre-processing\n","- Class weights calculation to balance the dataset distribution.\n","- Image segmentation to mask image background.\n","- Data augmentation to increase the images dataset.\n","\n","### 5. Processing\n","- Use of transfer learning with different pre-trained models like Resnet50 and InceptionV3. Other pre-trained models can be easily added.\n","- Use of custom CNN with multiple layers.\n","- FNN as the last layer classifier.\n","\n","### 6. Generate prediction file\n","- Generate prediction file with Kaggle competition format."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0SIwnKrsQsZt"},"source":["# 1. Kagglers challenges and discussions\n","The first step is to review the discussion forum for this competition to better discover the challenges and approaches in solving the problem.\n","\n","For a detailed discussion please refer [here](https://github.com/jprussoibanez/plant-seedlings-classification/blob/master/docs/kagglers_discussions.md).\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"evTpu-B-QsZu"},"source":["# 2. Libraries and settings\n","\n","## 2.1 Libraries\n","\n","This are the main libraries for coding the kernel:\n","1. [tensorflow](https://www.tensorflow.org/) with [keras](https://keras.io/) for managing the deep learning models. Using latest tensorflow 2.0 and tensorflow keras on google colab.\n","2. [sckilit-learn](https://scikit-learn.org/), [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/) with [seaborn](https://seaborn.pydata.org/) for data manipulation, analysis and visualization.\n","3. [TQDM](https://github.com/tqdm/tqdm) for progress bar visualization on processing.\n","4. [opencv](https://pypi.org/project/opencv-python/) for image processing."]},{"cell_type":"code","execution_count":0,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{},"colab_type":"code","id":"XJdpDXZYQsZv","trusted":true},"outputs":[],"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","# Keras\n","from tensorflow.keras.applications import vgg16, inception_v3, resnet50\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.regularizers import l1, l2, l1_l2\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, GlobalAveragePooling2D, Input, Activation, BatchNormalization, GlobalMaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n","\n","import os\n","\n","# Helpers\n","from tqdm import tqdm\n","from enum import Enum\n","\n","# Image processing\n","import cv2\n","\n","# Image plotting\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","%matplotlib inline\n","\n","# Sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.utils.class_weight import compute_class_weight"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r8LcDxHkQsZz"},"source":["## 2.2 Global variables\n","\n","This are the global variables use to setup the configuration for the models."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"qs5mCfiQQsZz","trusted":true},"outputs":[],"source":["# Global settings\n","TRAIN_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"train\")\n","TEST_DATASET_FOLDER = os.path.join(BASE_DATASET_FOLDER, \"test\")\n","IMAGE_WIDTH = 299\n","IMAGE_HEIGHT = 299\n","IMAGE_CHANNELS = 3\n","\n","TSNE_VISUALIZATION = True\n","\n","# Data augmentation settings\n","rotation_range = 250      \n","zoom_range = 0.5         \n","width_shift_range = 0.5  \n","height_shift_range = 0.5 \n","horizontal_flip = True   \n","vertical_flip = True\n","\n","# Training settings\n","batch_size = 30\n","epochs = 500\n","steps_per_epoch = 50\n","patience = 8\n","\n","class TrainingNetwork(Enum):\n","    CUSTOM_CNN = 1    # Custom multi-layer convolutional network with 3 layers\n","    RESNET_50 = 2     # Pre-trained Resnet50\n","    INCEPTION_V3 = 3  # Pre-trained InceptionV3\n","\n","# Choose model to train\n","NETWORK_TO_TRAIN = TrainingNetwork.CUSTOM_CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VUpWTJIOQsZ3"},"source":["# 3. Data analysis\n","\n","The first and the most important task in solving a problem with Machine Learning is to analyze the dataset before proceeding with any algorithms. This is important in order to understand the complexity of the dataset which will eventually help in designing the algorithm."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ky0fNzmqQsZ3"},"source":["## 3.1 Load data and images\n","\n","The first step is to load the images and data from the datasets through panda and dataframes."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"IYJgzEtUQsZ4","trusted":true},"outputs":[],"source":["def get_species_groups():\n","    \"\"\" \n","    Get the species group from the folders' name on the dataset. \n","  \n","    @Returns: \n","        array: Array of species groups to train.\n","    \"\"\"\n","    return [\n","      'Black-grass',\n","      'Charlock',\n","      'Cleavers',\n","      'Common Chickweed',\n","      'Common wheat',\n","      'Fat Hen',\n","      'Loose Silky-bent',\n","      'Maize',\n","      'Scentless Mayweed',\n","      'Shepherds Purse',\n","      'Small-flowered Cranesbill',\n","      'Sugar beet'\n","    ]\n","\n","def read_train_data(species_groups):\n","    \"\"\" \n","    Read the train species data from the datasets folder. \n","  \n","    @Parameters:\n","        species_groups (array): Array of species groups to train.\n","  \n","    @Returns: \n","        dataframe: Dataframe with the filepath, filename and species group category for each species image.\n","    \"\"\"\n","    train = []\n","    for _, species in tqdm(enumerate(species_groups), total=len(species_groups)):\n","        for file in os.listdir(os.path.join(TRAIN_DATASET_FOLDER, species)):\n","            train.append([f'{TRAIN_DATASET_FOLDER}/{species}/{file}', file, species])\n","    \n","    train_df = pd.DataFrame(train, columns=['filepath', 'file', 'species'])\n","    \n","    return train_df\n","\n","def read_image(image_file_path, image_width, image_height):\n","    \"\"\" \n","    Read and transform image according to widht, height and channels.\n","  \n","    @Parameters:\n","        image_file_path(string): Image file path to read.\n","        image_width (int): Image width to resize.\n","        image_height (int): Image height to resize.\n","  \n","    @Returns: \n","        array: Array of loaded images resized to image_width and image_height.\n","    \"\"\"\n","    image = cv2.imread(image_file_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_width, image_height))\n","    \n","    return image\n","\n","def load_images(filepaths, image_width, image_height, preprocess_function = lambda x: x):\n","    images =[]\n","    for filepath in tqdm(filepaths, total=len(filepaths)):\n","      images.append(preprocess_function(read_image(filepath, image_width, image_height)))\n","\n","    return images\n","    \n","def load_species_images(species_data, image_width, image_height, preprocess_function = lambda x: x):\n","    \"\"\" \n","    Load images to input features on neural network model and do one hot encode for labels.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        image_width (int): Image width to resize.\n","        image_height (int): Image height to resize.\n","       \n","    @Returns: \n","        species_X (array): Array of images on the data.\n","        species_Y (array): One hot encode for labels.\n","    \"\"\" \n","    species_X = load_images(species_data['filepath'],image_width, image_height, preprocess_function)\n","    species_Y = pd.get_dummies(species_data['species'], drop_first = False)\n","    \n","    return np.stack(species_X), species_Y\n","\n","def show_grid_multiple_sample(species_data, species_groups):\n","    \"\"\" \n","    Plot sample images from each species group.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        species_groups (array): Array with species group.\n","       \n","    @Returns: None\n","    Plots grid with number_of_samples images with image_size for each species_group\n","    \"\"\"\n","    number_of_samples = 8\n","    image_size = 100\n","    \n","    number_of_species = len(species_groups) \n","    fig, axes = plt.subplots(nrows = number_of_species, ncols = number_of_samples + 1, figsize = (20, 28), gridspec_kw = {'wspace': 0.05, 'hspace': 0})\n"," \n","    for species_id, species in enumerate(species_groups):\n","        samples_species_filepath = species_data[species_data['species'] == species]['filepath'].sample(number_of_samples)\n","        ax = axes[species_id, 0]\n","        ax.axis('off')\n","        ax.text(0.5, 0.5, species, horizontalalignment = 'center', verticalalignment = 'center', fontsize = 10, transform = ax.transAxes)\n","        for sample_id, sample_species_filepath in enumerate(samples_species_filepath):\n","            image = read_image(sample_species_filepath, image_size, image_size)\n","            ax = axes[species_id, sample_id + 1]\n","            ax.axis('off')\n","            ax.imshow(image)\n","            \n","    plt.show()\n","\n","def show_grid_one_sample(species_data, species_groups):\n","    \"\"\" \n","    Plot one sample image from each species group.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","        species_groups (array): Array with species group.\n","       \n","    @Returns: None\n","    Plots one sample image with image_size for each species_group\n","    \"\"\"\n","    image_size = 100\n","    number_of_species = len(species_groups)\n","    \n","    fig, axes = plt.subplots(nrows = 2, ncols = int(number_of_species / 2), figsize = (20, 8), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n","    axes = axes.flatten()\n","    \n","    for group_id, (group_name, group) in enumerate(species_data.groupby('species')):\n","        image = read_image(group.sample(1).iloc[0]['filepath'], image_size, image_size)\n","        axes[group_id].axis('off')\n","        axes[group_id].set_title(group_name)\n","        axes[group_id].imshow(image)\n","            \n","    plt.show()\n","    \n","def show_species_distribution(species_data):\n","    \"\"\" \n","    Plot species distribution.\n","  \n","    @Parameters:\n","        species_data (dataframe): Dataframe with the filepath, filename and species group category for each species image to load.\n","       \n","    @Returns: None\n","    Plots species distribution on an histogram and pie chart.\n","    \"\"\"\n","    species_groups = species_data['species'].unique()\n","    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n","    colors_palette = sns.husl_palette(len(species_groups), h = 0.01, l = 0.6, s = .2)\n","    explode_chart = np.full(len(species_groups), 0.1)\n","    \n","    species_data.groupby('species').size().plot.bar(\n","        subplots = True, ax = axes[0], stacked = True, title = 'Species distribution', label = \"\", color = colors_palette\n","    )\n","     \n","    species_data['species'].value_counts().plot.pie(\n","        subplots = True, ax = axes[1], autopct = '%.2f', title = 'Species distribution', label = \"\", colors = colors_palette, explode = explode_chart\n","    )\n","    \n","    plt.show()\n","    \n","class TSNE_dimension(Enum):\n","    TWO = 1\n","    THREE = 2\n","    \n","def apply_PCA(images, n_components = 100):\n","    pca = PCA(n_components = n_components)\n","    pca_result = pca.fit_transform(images)\n","    \n","    return pca_result\n","\n","def apply_TSNE(images, n_components = 2, perplexity = 100):\n","    tsne = TSNE(n_components = n_components, perplexity = perplexity)\n","    tsne_result = tsne.fit_transform(images)\n","    \n","    return tsne_result\n","\n","def plot_TSNE_with_PCA(images, image_labels, dimension = TSNE_dimension.TWO):\n","    pca_results = apply_PCA(images)\n","    n_components = (2 if dimension == TSNE_dimension.TWO else 3)\n","    tsne_result = apply_TSNE(pca_results, n_components)\n","    plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO)\n","\n","def plot_TSNE_results(tsne_result, image_labels, dimension = TSNE_dimension.TWO):\n","    df = pd.DataFrame(tsne_result).assign(label = image_labels)\n","    plt.figure(figsize=(10,10))\n","    if dimension == TSNE_dimension.TWO:\n","        sns.scatterplot(x = 0, y = 1, hue = 'label', data = df);\n","    else:\n","        fig = px.scatter_3d(df, x = 0, y = 1, z = 2, color = 'label')\n","        fig.show();"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dzC2PaiDQsZ8"},"source":["### 3.1.1 Load data\n","\n","Load data from the datasets image folders."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"D47VH-A2QsZ9","trusted":true},"outputs":[],"source":["species_groups = get_species_groups()\n","print(f\"This are the groups to be loaded and trained: {species_groups}\")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"_7bv4AqEQsZ_","trusted":true},"outputs":[],"source":["all_train_df = read_train_data(species_groups)\n","all_train_df.sample(4)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9aHsC_LjQsaC"},"source":["### 3.1.2 Descriptive analysis\n","\n","The descriptive analysis has the following steps:\n","1. Visualize sample images by species group.\n","2. Analyze species group distribution.\n","3. Apply dimensionality reduction through PCA & t-SNE."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2gKtKCyzQsaC"},"source":["#### 3.1.2.1 Sample images by species group\n","\n","The sample visualization shows the difficulty to distinguish between different weeds species groups even for a human eye."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"4EawzugWQsaD","trusted":true},"outputs":[],"source":["show_grid_multiple_sample(all_train_df, species_groups)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"fNL43B3fQsaG","trusted":true},"outputs":[],"source":["show_grid_one_sample(all_train_df, species_groups)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yvb4iB1IQsaJ"},"source":["#### 3.1.2.2 Species group distribution\n","\n","The distribution visualization demonstrates an imbalance in the dataset weed types or groups."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JQXjO1-AQsaJ","trusted":true},"outputs":[],"source":["show_species_distribution(all_train_df)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RPYPuI69QsaM"},"source":["#### 3.1.2.3 PCA & TSNE\n","\n","The PCA and t-SNE can be applied for dimensionality reduction to images on 2D and 3D. There is no clear clustering or similarity on the lower dimensions (at least before applying the pre-processing segmentation process).\n","\n","NOTE: The number of sample images are reduced to 500 images to avoid long loading times."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"pGj3RJ6pQsaN","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    sample_df = all_train_df.sample(500)\n","    sample_images, _ = load_species_images(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"YDmQIaUFQsaP","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    flatten_images = [image.flatten() for image in sample_images]\n","    pca_results = apply_PCA(flatten_images)\n","    tsne_results = apply_TSNE(pca_results)\n","    plot_TSNE_results(tsne_results, sample_df['species'].values)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"ElWm2h5UQsaR","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    tsne_results = apply_TSNE(pca_results, n_components = 3)\n","    plot_TSNE_results(tsne_results, sample_df['species'].values, TSNE_dimension.THREE)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tz9wRNIzQsaU"},"source":["## 3.2 Conclusions\n","\n","From the descriptive analysis some of the conclusions are:\n","- There is an imbalance on the dataset which should be handled to avoid training biases.\n","- There are not much images for training deep learning models so we should use data augmentation to increase the dataset.\n","- All weed images have backgrounds that we should be removed in order to improve weed recognition.\n","- There is no clear clustering on lower dimensions before pre-processing."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8LtwOJYGQsaW"},"source":["# 4. Pre-processing\n","\n","This are the principal concerns that should be addressed on the pre-processing from the data analysis:\n","\n","1. Balance the dataset to avoid biases on imbalance weed types.\n","2. Use image segmentation to remove background as to focus on weed recognition.\n","3. Apply data augmentation to increase the data available for training."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kmaO3tQ_QsaW"},"source":["## 4.1 Balance dataset\n","\n","The class weights computation allows to handle unbalance data distribution by weighting the class importance on training.\n","\n","Other ways of balancing the dataset are oversampling and undersampling with methods like SMOTE and ADASYN."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Hy_0XXHbQsaY","trusted":true},"outputs":[],"source":["def compute_class_weights(data_groups) :\n","    return dict(enumerate(compute_class_weight(\"balanced\", np.unique(data_groups), data_groups)))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"SUYTyRajQsaa","trusted":true},"outputs":[],"source":["class_weights = compute_class_weights(all_train_df['species'])\n","print(f\"This are the class weights that will be use for training to balance the dataset: {class_weights}\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8t72_VIpQsac"},"source":["## 4.2 Segmentation\n","The segmentation process masks the main object of analysis by removing all unnecessary background.\n","\n","In this section the segmentation operation is applied to a single image with the following steps:\n","\n","1. Mask creation \n","  1. Change the color space from RGB to HSV. This is an easier space to define color ranges.\n","  2. Define the color range for green (common weed color).\n","  3. Define the most common shape for weeds. For this case close ellipses.\n","  4. Apply the morphology transformation to identify the weeds within the color range and shape. \n","2. Apply mask to remove background\n","\n","Further information can be found on openCV documentation on [colorspaces](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html) and [Morphological Transformations](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"ntMbGjcNQsad","trusted":true},"outputs":[],"source":["def create_mask_for_plant(image):\n","    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    sensitivity = 35\n","    lower_hsv = np.array([60 - sensitivity, 100, 50])\n","    upper_hsv = np.array([60 + sensitivity, 255, 255])\n","\n","    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n","    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","    \n","    return mask\n","\n","def segment_plant(image):\n","    mask = create_mask_for_plant(image)\n","    output = cv2.bitwise_and(image, image, mask = mask)\n","    return output\n","\n","def create_image_tranformation(image):\n","    masked_image = create_mask_for_plant(image)\n","    segmented_image = segment_plant(image)\n","    \n","    return masked_image, segmented_image\n","\n","def load_species_images_with_segmentation(species_data, image_width, image_height):\n","    return load_species_images(species_data, image_width, image_height, segment_plant)\n","\n","def plot_image_transformation(original_image, masked_image, segmented_image):\n","    fig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(10, 10))\n","    axs[0].imshow(original_image)\n","    axs[0].set_title('Original image')\n","    axs[0].axis('off')\n","    axs[1].imshow(masked_image)\n","    axs[1].axis('off')\n","    axs[1].set_title('Masked image')\n","    axs[2].imshow(segmented_image)\n","    axs[2].set_title('Segmented image')\n","    axs[2].axis('off')"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"rk_SuutAQsaf","trusted":true},"outputs":[],"source":["sample_image, _ = load_species_images(all_train_df.sample(1), IMAGE_WIDTH, IMAGE_HEIGHT)\n","masked_image, segmented_image = create_image_tranformation(sample_image[0])\n","plot_image_transformation(sample_image[0], masked_image, segmented_image)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D2iokxpcQsah"},"source":["Applying t-SNE again on the same sample images but now with segmentation."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"kKHRFaFeQsai","trusted":true},"outputs":[],"source":["if TSNE_VISUALIZATION:\n","    sample_images, _ = load_species_images_with_segmentation(sample_df, IMAGE_WIDTH, IMAGE_HEIGHT)\n","    flatten_images = [image.flatten() for image in sample_images]\n","    plot_TSNE_with_PCA(flatten_images, sample_df['species'].values, TSNE_dimension.TWO)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gJOIOEYOQsal"},"source":["## 4.2 Data augmentation\n","\n","For the data augmentation we can use keras preprocessing library [ImageDataGenerator](https://keras.io/preprocessing/image/).\n","\n","The class can generate batches of tensor image data with real-time data augmentation while training the model. This are the transformations apply to augment the data:\n","- Random rotations within a range define by rotation_range.\n","- Random zooms within a range define by zoom_range.\n","- Random width shifts define by width_shift_range.\n","- Random heigh shifts define by height_shift_range.\n","- Random horizontal flips.\n","- Random vertical flips.\n","\n","NOTE: All this variables can be configured at the beginning of the notebook.\n","\n","This section will show some random transformations on a number of sample images."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"bvZFauvHQsam","trusted":true},"outputs":[],"source":["def create_image_data_generator(\n","    rotation_range,  \n","    zoom_range,\n","    width_shift_range, \n","    height_shift_range, \n","    horizontal_flip,\n","    vertical_flip\n","):\n","    return ImageDataGenerator(\n","        featurewise_center = False,\n","        samplewise_center = False,\n","        featurewise_std_normalization = False,\n","        samplewise_std_normalization = False,\n","        zca_whitening = False,\n","        rotation_range = rotation_range,\n","        zoom_range = zoom_range,\n","        width_shift_range = width_shift_range,\n","        height_shift_range = height_shift_range,\n","        horizontal_flip = horizontal_flip,\n","        vertical_flip = vertical_flip\n","    )\n","    \n","def show_augmentation_row(original_image, images_iterator, number_of_augmentations, axes):   \n","    axes[0].axis('off')\n","    axes[0].set_title('Original image')\n","    axes[0].imshow(original_image)\n","    \n","    for image_number in range(number_of_augmentations):\n","        image = images_iterator.next()\n","        axes[image_number + 1].axis('off')\n","        axes[image_number + 1].set_title('Augmented image')\n","        axes[image_number + 1].imshow(image[0].astype(np.uint8))\n","\n","def show_grid_with_augmented_images(number_of_samples, number_of_augmentations, image_data_generator):   \n","    fig, axes = plt.subplots(nrows = number_of_samples, ncols = number_of_augmentations + 1, figsize = (30, 4 * number_of_samples), gridspec_kw = {'wspace': 0.05, 'hspace': 0.05})\n","    fig.suptitle('Sample augmented images', fontsize=16)\n","    \n","    sample_images, _ = load_species_images_with_segmentation(all_train_df.sample(number_of_samples), IMAGE_WIDTH, IMAGE_HEIGHT)\n","    for sample_id, sample_image in enumerate(sample_images):\n","        images_iterator = image_data_generator.flow(np.expand_dims(sample_image, axis=0), None, 1)\n","        show_augmentation_row(sample_image, images_iterator, number_of_augmentations, axes[sample_id])\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Qj2wffkLQsao","trusted":true},"outputs":[],"source":["image_data_generator = create_image_data_generator(\n","    rotation_range = 250,    \n","    zoom_range = 0.2,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    horizontal_flip = True,\n","    vertical_flip = True,\n",")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"9Erg4XixQsaq","trusted":true},"outputs":[],"source":["show_grid_with_augmented_images(number_of_samples = 5, number_of_augmentations = 7, image_data_generator = image_data_generator)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m0irIOlPQsas"},"source":["# 5. Processing\n","\n","This section describes the model architecture and the training process. This are the following architectures use for training:\n","\n","- Pre-trained models: This models are use for transfer learning by reusing their weights/layers and retraining only the classifier for the specific domain. \n","- Custom CNN: This is a multi-layer CNN using batch normalization and regularization.\n","\n","Also different metrics are use to validate the model like accuracy, recall, f1-score and the confusion matrix.\n","\n","The training set is divided in training (80%), validation (10%) and test sets (10%).\n","\n","The NETWORK_TO_TRAIN parameter (at the beginning of the notebook) can be used to select the model to train:\n","\n","- CUSTOM_CNN: Custom multi-layer CNN with FNN classifier.\n","- RESNET_50: Resnet_50 model with FNN classifier.\n","- INCEPTION_V3: InceptionV3 model with FNN classifier.\n","\n","It should be easy to add additional pre-trained models if desired.\n","\n","Some additional improvements, not implemented on this notebook yet, are:\n","\n","- Fine tune pre-trained models by unfreezing and training the last layers on the CNN.\n","- Use hyperparameters optimization using some library like hyperopt to optimize model parameters.\n","- Use more classifiers like XGBoost, SVM, etc. instead of just the FNN.\n","- Ensemble models to improve performance by combining different models.\n","- Use cross-validation to better evaluate the estimator performance."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"lc4DJJRRQsat","trusted":true},"outputs":[],"source":["number_output_classes = len(get_species_groups())"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"2T7CjMIDQsav","trusted":true},"outputs":[],"source":["class TrainingModel:\n","    accuracy_metric = {\n","        'training': {\n","            'metric': 'accuracy',\n","            'label': 'Accuracy'\n","        },\n","        'validation': {\n","            'metric': 'val_accuracy',\n","            'label': 'Validation accuracy'\n","        }\n","    }\n","    loss_metric = {\n","        'training': {\n","            'metric': 'loss',\n","            'label': 'Loss'\n","        },\n","        'validation': {\n","            'metric': 'val_loss',\n","            'label': 'Validation loss'\n","        }\n","    }\n","    \n","    def __init__(self, image_data_generator, base_model, classifier_model):\n","        self.image_data_generator = image_data_generator\n","        self.optimizer = Adam(lr=1e-3)\n","        self.loss = 'categorical_crossentropy'\n","        self.metrics = [TrainingModel.accuracy_metric['training']['metric']]\n","        self.training_results = None\n","        self.training_model = self.create_model(base_model, classifier_model)\n","        self.species_names = get_species_groups()\n","    \n","    def __del__(self):\n","        del self.image_data_generator\n","        del self.training_model\n","        del self.training_results\n","    \n","    def create_model(self, base_model, classifier_model):\n","        model = Sequential(name = \"Species-Prediction-CNN\")\n","        model.add(base_model)\n","        model.add(classifier_model)\n","\n","        model.compile(optimizer = self.optimizer, loss = self.loss, metrics = self.metrics)  \n","        model.summary()\n","\n","        return model\n","\n","    def train_model(\n","        self,\n","        train_X,\n","        train_y,\n","        validation_data_X,\n","        validation_data_y,\n","        batch_size,\n","        epochs,\n","        steps_per_epoch,\n","        patience,\n","        class_weights\n","    ):\n","    \n","        self.image_data_generator.fit(train_X)\n","        earlystopper = EarlyStopping(monitor = 'loss', patience = patience, verbose = 1, restore_best_weights = True)\n","        lr_reduce = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_delta = 1e-5, patience = patience, verbose = 1)\n","\n","        self.training_results = self.training_model.fit(\n","            self.image_data_generator.flow(train_X, train_y, batch_size),\n","            epochs = epochs,\n","            validation_data = [validation_data_X, validation_data_y],\n","            steps_per_epoch = steps_per_epoch,\n","            callbacks = [earlystopper, lr_reduce],\n","            class_weight = class_weights\n","        )\n","        \n","    def predict(self, x):\n","        return self.training_model.predict(x, verbose = 1)\n","    \n","    def predict_species_class(self, x):\n","        return np.argmax(self.predict(x), axis = 1)\n","    \n","    def predict_species_name(self, x):\n","        return [self.species_names[species_class] for species_class in self.predict_species_class(x)]\n","\n","    def eval_model(self, test_X, test_Y, field_name = 'species'):\n","        \"\"\"\n","        Model evaluation: plots, classification report\n","        @param training: model training history\n","        @param model: trained model\n","        @param test_X: features \n","        @param test_y: labels\n","        @param field_name: label name to display on plots\n","        \"\"\"        \n","        test_pred = self.training_model.predict(test_X)\n"," \n","        self.plot_metrics(test_X, test_Y, test_pred, field_name)\n","        \n","        self.plot_classification_report(test_X, test_Y, test_pred)\n","        \n","        self.plot_confusion_matrix(test_Y, test_pred)\n","    \n","    def plot_metrics(self, test_X, test_y, test_pred, field_name = 'species'):\n","        fig, axes = plt.subplots(1, 3, figsize = (20, 7))\n","        \n","        self.plot_metric(TrainingModel.accuracy_metric, axes[0], field_name)\n","        self.plot_metric(TrainingModel.loss_metric, axes[1], field_name)\n","        self.plot_accuracy_by_species_group(test_X, test_y, test_pred, axes[2], field_name)\n","\n","        plt.tight_layout()\n","        plt.show()\n","        \n","    def plot_metric(self, metric, axes, field_name):\n","        axes.plot(self.training_results.history[metric['training']['metric']], label = metric['training']['label'])\n","        axes.plot(self.training_results.history[metric['validation']['metric']], label = metric['validation']['label'])\n","        axes.set_title(f'{field_name} {metric[\"training\"][\"metric\"]}')\n","        axes.set_xlabel('Epoch')\n","        axes.set_ylabel(metric['training']['label'])\n","        axes.legend()\n"," \n","    def plot_accuracy_by_species_group(self, test_X, test_y, test_pred, axes, field_name):\n","        acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum() / test_y.sum()\n","        acc_by_subspecies.plot.bar(title = f'Accuracy by {field_name}', ax = axes)\n","        plt.ylabel('Accuracy')\n","    \n","    def plot_classification_report(self, test_X, test_y, test_pred):\n","        print(\"Classification report\")\n","        test_pred = np.argmax(test_pred, axis = 1)\n","        test_truth = np.argmax(test_y.values, axis = 1)\n","\n","        print(metrics.classification_report(test_truth, test_pred, target_names = test_y.columns, zero_division = True))\n","\n","        test_res = self.training_model.evaluate(test_X, test_y.values, verbose = 0)\n","        print('Loss function: %s, accuracy:' % test_res[0], test_res[1])\n","        \n","    def plot_confusion_matrix(self, test_Y, test_pred):\n","        cnf_matrix = metrics.confusion_matrix(np.argmax(test_Y.values, axis = 1), np.argmax(test_pred, axis = 1))\n","\n","        abbreviation = ['Bg', 'Ch', 'Cl', 'CC', 'Cw', 'FH', 'LSb', 'M', 'SM', 'SP', 'SfC', 'Sb']\n","        pd.DataFrame({'class': self.species_names, 'abbreviation': abbreviation})\n","\n","        fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n","        ax = sns.heatmap(cnf_matrix, ax = ax, cmap = plt.cm.Greens, annot = True)\n","        ax.set_xticklabels(abbreviation)\n","        ax.set_yticklabels(abbreviation)\n","        plt.title('Confusion matrix of test set')\n","        plt.ylabel('True species')\n","        plt.xlabel('Predicted species')\n","        \n","        plt.show();"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"xqK4KaHAQsax","trusted":true},"outputs":[],"source":["def split(data):\n","    train_data, validation_data = train_test_split(data, test_size = 0.2, shuffle = True)\n","    validation_data, test_data = train_test_split(validation_data, test_size = 0.5, shuffle = True)\n","\n","    return (train_data, validation_data, test_data)\n","        \n","def create_CNN_classifier_global_max_pooling(batch_normalization = False, dropout = False, kernel_regularizer = None):\n","    model = Sequential(name = 'GlobalMaxPooling-FNN-classifier')\n","    model.add(GlobalMaxPooling2D())\n","    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense1\", activation = 'relu'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense2\", activation = 'relu'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(number_output_classes, activation = 'softmax', name = 'predictions'))\n","        \n","    return model\n","\n","def create_CNN_classifier_with_flatten(batch_normalization = False, dropout = False, kernel_regularizer = None):\n","    model = Sequential(name = 'Flatten-FNN-classifier')\n","    model.add(Flatten())\n","    model.add(Dense(1024, activation = \"relu\", kernel_regularizer = kernel_regularizer, name='dense1'))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Dropout(0.5)) if (dropout) else False\n","    model.add(Dense(number_output_classes, activation = \"softmax\", name='predictions'))\n","    \n","    return model\n","    \n","def create_inceptionV3_model():\n","    base_model = inception_v3.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_vgg16_model():\n","    base_model = vgg16.VGG16(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","     \n","    return base_model\n","\n","def create_resnet50_model():\n","    base_model = resnet50.ResNet50(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    return base_model\n","\n","def create_densenet201_model():\n","    base_model = densenet.DenseNet201(weights = 'imagenet', include_top = False, input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n","    for layer in base_model.layers:\n","        layer.trainable = False\n"," \n","    return base_model\n","\n","def create_simple_convolutional_base(batch_normalization = False):\n","    model = Sequential(name = 'Three-Layer-CNN-base')\n","    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), name=\"conv_layer_1.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', name=\"conv_layer_1.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (3, 3), strides = (2, 2)))\n","    \n","    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_2.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_2.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (3, 3), strides = (2, 2)))\n","        \n","    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_3.1\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', name = \"conv_layer_3.2\"))\n","    model.add(BatchNormalization()) if (batch_normalization) else False\n","    model.add(MaxPool2D(pool_size = (3, 3), strides = (2, 2)))\n","    \n","    return model\n","    "]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"pHowp1l4Qsaz","trusted":true},"outputs":[],"source":["train_data, validation_data, test_data = split(all_train_df)\n","\n","train_X, train_Y = load_species_images_with_segmentation(train_data, IMAGE_WIDTH, IMAGE_HEIGHT)\n","validation_X, validation_Y = load_species_images_with_segmentation(validation_data, IMAGE_WIDTH, IMAGE_HEIGHT)\n","test_X, test_Y = load_species_images_with_segmentation(test_data, IMAGE_WIDTH, IMAGE_HEIGHT)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"7FBc5BUVQsa1","trusted":true},"outputs":[],"source":["image_data_generator = create_image_data_generator(\n","    rotation_range,\n","    zoom_range,\n","    width_shift_range,\n","    height_shift_range,\n","    horizontal_flip,\n","    vertical_flip\n",")\n","\n","if NETWORK_TO_TRAIN == TrainingNetwork.CUSTOM_CNN:\n","    base_model = create_simple_convolutional_base(batch_normalization = True)\n","if NETWORK_TO_TRAIN == TrainingNetwork.RESNET_50:\n","    base_model = create_resnet50_model()\n","if NETWORK_TO_TRAIN == TrainingNetwork.INCEPTION_V3:\n","    base_model = create_resnet50_model()\n","\n","classifier_model = create_CNN_classifier_global_max_pooling(batch_normalization = True, dropout = True, kernel_regularizer = l2(0.01))\n","model = TrainingModel(image_data_generator, base_model, classifier_model)\n","model.train_model(\n","    train_X,\n","    train_Y,\n","    validation_X,\n","    validation_Y,\n","    batch_size,\n","    epochs,\n","    steps_per_epoch,\n","    patience,\n","    class_weights\n",")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"AhcblxCTQsa4","trusted":true},"outputs":[],"source":["model.eval_model(test_X, test_Y, \"species\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BY_-wQOWQsa7"},"source":["# 6. Generate prediction file\n","This section generates the prediction file to upload for Kaggle."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"oxAeQ9TOQsa8","trusted":true},"outputs":[],"source":["def load_test():\n","    test = []\n","    files = os.listdir(TEST_DATASET_FOLDER)\n","    for file in tqdm(files, total = len(files)):\n","        test.append([f'{TEST_DATASET_FOLDER}/{file}', file])\n","    \n","    test_df = pd.DataFrame(test, columns=['filepath', 'file'])\n","    \n","    return test_df\n","\n","def generate_prediction_file(model, image_width, image_height, preprocess_function = segment_plant):\n","    test_df = load_test()\n","    test_images = load_images(test_df['filepath'], image_width, image_height, preprocess_function)\n","    \n","    species_name_predictions = model.predict_species_name(np.stack(test_images))\n","    file_output = np.column_stack((test_df['file'], species_name_predictions))\n","    \n","    file_df = pd.DataFrame(file_output, columns = ['file', 'species'])\n","    file_df.to_csv(\"submission.csv\", index = False, index_label = False)\n","    \n","    return file_df"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"7AYqIdnVQsa-","trusted":true},"outputs":[],"source":["file_df = generate_prediction_file(model, IMAGE_WIDTH, IMAGE_HEIGHT)\n","file_df.head(10)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ojBjaPxMQsbA"},"source":["## Learnings\n","\n","1. Skimage could not handle the images loading with 16 GB RAM limit so OpenCV was used instead.\n","2. t-SNE improve after applying segmentation and masking over the images, but still cannot cluster correctly on lower dimensions.\n","3. Tensorflow and keras libraries were upgraded after migrating from Kaggle to Google Colab. Kaggle does not have unlimited GPU usage anymore."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fyurinPoQsbB"},"source":["# Links\n","\n","This are some papers and links use during the exercise resolution:\n","\n","- [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239.pdf)\n","- [A New Design Based-SVM of the CNN Classifier Architecture with Dropout for Offline Arabic Handwritten Recognition](https://www.sciencedirect.com/science/article/pii/S1877050916309991)\n","- [Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)\n","- [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)\n","- [Regularization and Optimization strategies in Deep Convolutional Neural Network](https://arxiv.org/pdf/1712.04711.pdf)\n","- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","- [Revisiting small batch training for deep neural networks](https://arxiv.org/pdf/1804.07612.pdf)"]}]}